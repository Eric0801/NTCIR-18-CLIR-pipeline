{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1743104e",
   "metadata": {},
   "source": [
    "## Step 0: Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b313258",
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/Eric0801/NTCIR-18-CLIR-pipeline-team6939.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3ae35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install transformers rank_bm25 sentence-transformers faiss-cpu jieba tqdm opencc pymupdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abff5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 setup_models.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00332849",
   "metadata": {},
   "source": [
    "## Step 1: Intialize Models\n",
    "\n",
    "if models haven't installed, it will check and install to \"models/\" directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d24587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: Initialize Models (Colab Local)\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "\n",
    "def download_model(name, hf_id, is_classifier=True):\n",
    "    save_dir = Path(\"models\") / name\n",
    "    if save_dir.exists() and any(save_dir.iterdir()):\n",
    "        print(f\"[✓] {name} already exists, skipping download.\")\n",
    "        return\n",
    "    print(f\"↓ Downloading {name} from HuggingFace...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(hf_id)\n",
    "    model_cls = AutoModelForSequenceClassification if is_classifier else AutoModel\n",
    "    model = model_cls.from_pretrained(hf_id)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    tokenizer.save_pretrained(save_dir)\n",
    "    model.save_pretrained(save_dir)\n",
    "    print(f\"[✓] {name} saved to {save_dir}\")\n",
    "\n",
    "download_model(\"zhbert\", \"hfl/chinese-roberta-wwm-ext\", is_classifier=True)\n",
    "download_model(\"labse\", \"sentence-transformers/LaBSE\", is_classifier=False)\n",
    "download_model(\"cross_encoder\", \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\", is_classifier=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89ab939",
   "metadata": {},
   "source": [
    "## Step 2: Extract Paragraphs from PDF (PyMuPDF + EasyOCR fallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a4e081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import easyocr\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from pdf2image import convert_from_path\n",
    "from pathlib import Path\n",
    "import traceback\n",
    "import glob\n",
    "\n",
    "# Install required poppler-utils if not already installed\n",
    "!apt-get update -qq && apt-get install -qq -y poppler-utils\n",
    "\n",
    "# Base directory settings\n",
    "BASE_DIR = Path(\"/content/NTCIR-18-CLIR-pipeline-team6939\")\n",
    "PDF_DIRS = [\n",
    "    BASE_DIR / \"pdfs/finance\",\n",
    "    BASE_DIR / \"pdfs/insurance\"\n",
    "]\n",
    "OUTPUT_DIR = BASE_DIR / \"outputs\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize EasyOCR\n",
    "try:\n",
    "    print(\"Initializing EasyOCR...\")\n",
    "    reader = easyocr.Reader(['ch_tra', 'en'], gpu=False)\n",
    "    print(\"✓ EasyOCR initialized successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to initialize EasyOCR: {e}\")\n",
    "    print(\"Will try to process without OCR fallback\")\n",
    "    reader = None\n",
    "\n",
    "def extract_blocks_with_heuristics(pdf_path, min_block_length=40):\n",
    "    print(f\"Extracting text blocks using PyMuPDF from: {pdf_path}\")\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        results = []\n",
    "        doc_id = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "\n",
    "        for page_num, page in enumerate(doc):\n",
    "            blocks = page.get_text(\"blocks\")\n",
    "            for i, block in enumerate(sorted(blocks, key=lambda b: b[1])):\n",
    "                x0, y0, x1, y1, text, *_ = block\n",
    "                clean_text = text.strip().replace(\"\\n\", \" \")\n",
    "                if len(clean_text) >= min_block_length:\n",
    "                    results.append({\n",
    "                        \"pid\": f\"{doc_id}_p{page_num}_b{i}\",\n",
    "                        \"page\": page_num,\n",
    "                        \"bbox\": [x0, y0, x1, y1],\n",
    "                        \"text\": clean_text\n",
    "                    })\n",
    "\n",
    "        print(f\"✓ Extracted {len(results)} text blocks\")\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"PyMuPDF extraction failed: {e}\")\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "def fallback_ocr_easyocr(pdf_path):\n",
    "    print(f\"Using EasyOCR for fallback OCR processing: {pdf_path}\")\n",
    "    if reader is None:\n",
    "        print(\"Cannot perform OCR: EasyOCR not initialized\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        images = convert_from_path(pdf_path, dpi=300)\n",
    "        results = []\n",
    "        doc_id = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "\n",
    "        for page_num, image in enumerate(images):\n",
    "            print(f\"Processing page {page_num + 1}...\")\n",
    "\n",
    "            # Convert PIL Image to numpy array (format that EasyOCR expects)\n",
    "            image_np = np.array(image)\n",
    "\n",
    "            # Process with EasyOCR\n",
    "            try:\n",
    "                ocr_result = reader.readtext(image_np)\n",
    "                full_text = \" \".join([res[1] for res in ocr_result if len(res[1].strip()) > 0])\n",
    "\n",
    "                if full_text.strip():\n",
    "                    results.append({\n",
    "                        \"pid\": f\"{doc_id}_ocr_{page_num}\",\n",
    "                        \"page\": page_num,\n",
    "                        \"bbox\": None,\n",
    "                        \"text\": full_text.strip()\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"Error during OCR on page {page_num + 1}: {e}\")\n",
    "                continue\n",
    "\n",
    "        print(f\"✓ OCR processing completed, extracted {len(results)} pages of text\")\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"OCR processing failed: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return []\n",
    "\n",
    "def process_pdf_file(pdf_path):\n",
    "    print(f\"Processing PDF file: {pdf_path}\")\n",
    "\n",
    "    # Check if file exists\n",
    "    if not os.path.exists(pdf_path):\n",
    "        print(f\"Error: PDF file does not exist: {pdf_path}\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        # First try with PyMuPDF\n",
    "        segments = extract_blocks_with_heuristics(pdf_path)\n",
    "\n",
    "        # Skip OCR if PyMuPDF produced good results\n",
    "        if segments and not all(len(seg['text']) < 40 for seg in segments):\n",
    "            return segments\n",
    "\n",
    "        print(f\"PyMuPDF extraction results poor or empty, attempting OCR\")\n",
    "        ocr_segments = fallback_ocr_easyocr(pdf_path)\n",
    "\n",
    "        # If OCR also failed, return whatever we got from PyMuPDF\n",
    "        if not ocr_segments:\n",
    "            print(\"OCR produced no results, returning PyMuPDF results instead\")\n",
    "            return segments\n",
    "\n",
    "        return ocr_segments\n",
    "    except Exception as e:\n",
    "        print(f\"Error during PyMuPDF processing: {e}\")\n",
    "        print(\"Attempting OCR fallback\")\n",
    "\n",
    "        try:\n",
    "            return fallback_ocr_easyocr(pdf_path)\n",
    "        except Exception as e2:\n",
    "            print(f\"OCR fallback also failed: {e2}\")\n",
    "            print(\"Returning empty results for this PDF\")\n",
    "            return []\n",
    "\n",
    "# Find all PDF files in specified directories\n",
    "all_pdf_files = []\n",
    "for pdf_dir in PDF_DIRS:\n",
    "    if pdf_dir.exists():\n",
    "        pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "        all_pdf_files.extend(pdf_files)\n",
    "        print(f\"Found {len(pdf_files)} PDF files in {pdf_dir}\")\n",
    "    else:\n",
    "        print(f\"Warning: Directory does not exist: {pdf_dir}\")\n",
    "\n",
    "print(f\"Total PDFs found: {len(all_pdf_files)}\")\n",
    "\n",
    "# Process a subset of PDFs if there are too many (optional)\n",
    "MAX_PDFS = 99999999999  # Adjust this number as needed\n",
    "if len(all_pdf_files) > MAX_PDFS:\n",
    "    print(f\"Processing first {MAX_PDFS} PDFs out of {len(all_pdf_files)}\")\n",
    "    all_pdf_files = all_pdf_files[:MAX_PDFS]\n",
    "\n",
    "# Process all PDF files\n",
    "all_results = []\n",
    "successful_pdfs = 0\n",
    "for pdf_file in all_pdf_files:\n",
    "    print(f\"\\nProcessing {pdf_file}... ({successful_pdfs+1}/{len(all_pdf_files)})\")\n",
    "    try:\n",
    "        pdf_results = process_pdf_file(pdf_file)\n",
    "        all_results.extend(pdf_results)\n",
    "        print(f\"Extracted {len(pdf_results)} segments from {pdf_file}\")\n",
    "        successful_pdfs += 1\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {pdf_file}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "# Save all results to a single file\n",
    "output_file = OUTPUT_DIR / \"structured_passages.jsonl\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for r in all_results:\n",
    "        json.dump(r, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"✅ Extraction completed, results saved to: {output_file}\")\n",
    "print(f\"Total extracted passages: {len(all_results)}\")\n",
    "print(f\"Successfully processed {successful_pdfs}/{len(all_pdf_files)} PDFs\")\n",
    "\n",
    "# Display some sample text\n",
    "if all_results:\n",
    "    print(\"\\nSample texts:\")\n",
    "    for i, r in enumerate(all_results[:3]):  # Show only first 3\n",
    "        print(f\"[{i+1}] {r['pid']}: {r['text'][:100]}...\")\n",
    "else:\n",
    "    print(\"No text was extracted from any PDF.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93703d3",
   "metadata": {},
   "source": [
    "## Step 3: Hugging Face Translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce7643d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from opencc import OpenCC  # 新增：簡轉繁\n",
    "\n",
    "# 下載 HuggingFace 模型 (英文 ➔ 中文)\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-zh\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# 初始化簡轉繁工具\n",
    "cc = OpenCC('s2t')  # Simplified Chinese ➔ Traditional Chinese\n",
    "\n",
    "# 載入 queries\n",
    "with open(\"/content/NTCIR-18-CLIR-pipeline-team6939/data/translated_query.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    queries = json.load(f)\n",
    "\n",
    "# 設定 cache 檔案路徑（根據 model 名稱自動命名）\n",
    "cache_path = f\"/content/NTCIR-18-CLIR-pipeline-team6939/outputs/translated_cache_{model_name.replace('/', '_')}.json\"\n",
    "if os.path.exists(cache_path):\n",
    "    with open(cache_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        translated_cache = json.load(f)\n",
    "else:\n",
    "    translated_cache = {}\n",
    "\n",
    "# 定義 NMT 翻譯函式（支援 cache + 自訂是否轉繁體）\n",
    "def translate_with_nmt(query_en, convert_to_traditional=True):\n",
    "    if query_en in translated_cache:\n",
    "        return translated_cache[query_en]\n",
    "    try:\n",
    "        inputs = tokenizer(query_en, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            translated = model.generate(**inputs, max_length=512, num_beams=5)\n",
    "        result = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "        if convert_to_traditional:\n",
    "            result = cc.convert(result)  # 自動簡體轉繁體\n",
    "        translated_cache[query_en] = result\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Translation error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# 執行批次翻譯\n",
    "translated_output = []\n",
    "for item in tqdm(queries):\n",
    "    zh = translate_with_nmt(item[\"query_en\"], convert_to_traditional=True)  # 這邊可以控管要不要轉繁體\n",
    "    item[\"query_zh_nmt\"] = zh\n",
    "    translated_output.append(item)\n",
    "\n",
    "# 存檔：翻譯後查詢 + 快取\n",
    "os.makedirs(\"/content/NTCIR-18-CLIR-pipeline-team6939/outputs\", exist_ok=True)\n",
    "\n",
    "with open(\"/content/NTCIR-18-CLIR-pipeline-team6939/outputs/translated_query_nmt.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(translated_output, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "with open(cache_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(translated_cache, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"✅ HuggingFace NMT 簡繁轉換翻譯完成並成功快取保存。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f3a05c",
   "metadata": {},
   "source": [
    "## Step 4: Run Retrieval (4 Models with Runtime Logging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec4f751",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%cd /content/NTCIR-18-CLIR-pipeline-team6939\n",
    "from run_all_retrievals import run_all_retrievals\n",
    "run_all_retrievals()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc570911",
   "metadata": {},
   "source": [
    "#### 目前 zhbert reranker 跑不出來，但因為你說要 fine tune 所以我先跳過他"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad503e56",
   "metadata": {},
   "source": [
    "## Step 5: 合併結果成 retrieval_rankings.json ，然後評分（評分的 code 有大改，因為原本切太細了，等你修好之後 evaluate 也要再改）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdcd8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# ──────────────────────────────────────────────────\n",
    "# 1️⃣ 設定路徑（把 retrieval_rankings.json 放到 outputs/runs）\n",
    "# ──────────────────────────────────────────────────\n",
    "BASE         = Path(\"/content/NTCIR-18-CLIR-pipeline-team6939\")\n",
    "RUNS_DIR     = BASE/\"outputs\"/\"runs\"\n",
    "OUT_RANK     = RUNS_DIR/\"retrieval_rankings.json\"\n",
    "GROUND_TRUTH = BASE/\"data\"/\"ground_truths_example.json\"\n",
    "CSV_OUT      = BASE/\"outputs\"/\"evaluation_summary.csv\"\n",
    "\n",
    "# 確保 runs 資料夾存在\n",
    "RUNS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ──────────────────────────────────────────────────\n",
    "# 2️⃣ 讀三條 pipeline 的 jsonl，group by model→qid→[pid]\n",
    "# ──────────────────────────────────────────────────\n",
    "retrieval = {}\n",
    "for model_name in [\"bm25_only\", \"dense_dual_encoder\", \"cross_encoder\"]:\n",
    "    fn       = RUNS_DIR/f\"{model_name}.jsonl\"\n",
    "    grouping = defaultdict(list)\n",
    "    with open(fn, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            r = json.loads(line)\n",
    "            grouping[str(r[\"qid\"])].append(r[\"pid\"])\n",
    "    retrieval[model_name] = grouping\n",
    "\n",
    "# ──────────────────────────────────────────────────\n",
    "# 3️⃣ 寫出 retrieval_rankings.json 到 outputs/runs\n",
    "# ──────────────────────────────────────────────────\n",
    "with open(OUT_RANK, 'w', encoding='utf-8') as f:\n",
    "    json.dump(retrieval, f, indent=2, ensure_ascii=False)\n",
    "print(f\"✅ grouped rankings saved to {OUT_RANK}\")\n",
    "\n",
    "# ──────────────────────────────────────────────────\n",
    "# 4️⃣ 呼叫 evaluate_all_models()\n",
    "# ──────────────────────────────────────────────────\n",
    "# 把 src 加到 module path\n",
    "sys.path.append(str(BASE/\"src\"))\n",
    "\n",
    "from evaluation.evaluation_summary import evaluate_all_models\n",
    "\n",
    "df = evaluate_all_models(\n",
    "    ranking_path=str(OUT_RANK),\n",
    "    ground_truth_path=str(GROUND_TRUTH),\n",
    "    output_csv_path=str(CSV_OUT),\n",
    "    ks=[10, 100]\n",
    ")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb3b0f8",
   "metadata": {},
   "source": [
    "                Model  MRR  Recall@10  NDCG@10  Recall@100  NDCG@100\n",
    "0           bm25_only  0.0        0.0      0.0         0.0       0.0\n",
    "1  dense_dual_encoder  0.0        0.0      0.0         0.0       0.0\n",
    "2       cross_encoder  0.0        0.0      0.0         0.0       0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73572e32",
   "metadata": {},
   "source": [
    "## Step 6: Translation Error Impact Analysis (還沒試過)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022e41cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from translate_error_analysis import extract_translation_impact\n",
    "\n",
    "impact = extract_translation_impact(\n",
    "    queries_path=\"data/translated_query.json\",\n",
    "    predictions_path=\"outputs/retrieval_rankings.json\",\n",
    "    ground_truth_path=\"data/ground_truths_example.json\"\n",
    ")\n",
    "\n",
    "for category, group in impact.items():\n",
    "    print(f\"\\n== {category.upper()} ({len(group)} samples) ==\")\n",
    "    for qid, en, zh, pred, gt in group[:1]:\n",
    "        print(f\"QID: {qid}\\nEN: {en}\\nZH: {zh}\\nPRED: {pred}\\nGT: {gt}\\n---\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
