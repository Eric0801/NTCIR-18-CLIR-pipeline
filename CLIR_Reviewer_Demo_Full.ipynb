{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "660916e2",
   "metadata": {},
   "source": [
    "#### DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b313258",
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/Eric0801/NTCIR-18-CLIR-pipeline-team6939.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df3ae35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages (4.51.3)\n",
      "Requirement already satisfied: rank_bm25 in /Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages (0.2.2)\n",
      "Requirement already satisfied: sentence-transformers in /Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages (4.1.0)\n",
      "Requirement already satisfied: faiss-cpu in /Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages (1.11.0)\n",
      "Requirement already satisfied: jieba in /Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages (0.42.1)\n",
      "Requirement already satisfied: tqdm in /Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages (4.67.1)\n",
      "Requirement already satisfied: opencc in /Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages (1.1.9)\n",
      "Requirement already satisfied: pymupdf in /Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages (1.25.5)\n",
      "Requirement already satisfied: python-dotenv in /Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages (1.1.0)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement faiss (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for faiss\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install transformers rank_bm25 sentence-transformers faiss-cpu jieba tqdm opencc pymupdf python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1abff5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "[✓] HuggingFace private token loaded.\n",
      "[✓] zhbert already exists, skipping.\n",
      "↓ Downloading labse from sentence-transformers/LaBSE...\n",
      "modules.json: 100%|████████████████████████████| 461/461 [00:00<00:00, 1.22MB/s]\n",
      "config_sentence_transformers.json: 100%|████████| 122/122 [00:00<00:00, 570kB/s]\n",
      "README.md: 100%|███████████████████████████| 2.02k/2.02k [00:00<00:00, 11.0MB/s]\n",
      "sentence_bert_config.json: 100%|██████████████| 53.0/53.0 [00:00<00:00, 200kB/s]\n",
      "config.json: 100%|█████████████████████████████| 190/190 [00:00<00:00, 1.37MB/s]\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "config.json: 100%|██████████████████████████████| 114/114 [00:00<00:00, 788kB/s]\n",
      "pytorch_model.bin:   0%|                            | 0.00/2.36M [00:00<?, ?B/s]\n",
      "pytorch_model.bin: 100%|███████████████████| 2.36M/2.36M [00:00<00:00, 4.06MB/s]\u001b[A\n",
      "\n",
      "model.safetensors: 100%|███████████████████| 2.36M/2.36M [00:00<00:00, 3.98MB/s]\u001b[A\n",
      "[✓] Saved to models/labse\n",
      "[✓] cross_encoder already exists, skipping.\n",
      "[✓] zhbert_finetuned-v2 already exists, skipping.\n"
     ]
    }
   ],
   "source": [
    "!python3 setup_models.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00332849",
   "metadata": {},
   "source": [
    "## Step 1: Intialize Models\n",
    "\n",
    "if models haven't installed, it will check and install to \"models/\" directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0d24587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] zhbert already exists, skipping download.\n",
      "[✓] labse already exists, skipping download.\n",
      "[✓] cross_encoder already exists, skipping download.\n"
     ]
    }
   ],
   "source": [
    "# Step 0: Initialize Models (Colab Local)\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "\n",
    "def download_model(name, hf_id, is_classifier=True):\n",
    "    save_dir = Path(\"models\") / name\n",
    "    if save_dir.exists() and any(save_dir.iterdir()):\n",
    "        print(f\"[✓] {name} already exists, skipping download.\")\n",
    "        return\n",
    "    print(f\"↓ Downloading {name} from HuggingFace...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(hf_id)\n",
    "    model_cls = AutoModelForSequenceClassification if is_classifier else AutoModel\n",
    "    model = model_cls.from_pretrained(hf_id)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    tokenizer.save_pretrained(save_dir)\n",
    "    model.save_pretrained(save_dir)\n",
    "    print(f\"[✓] {name} saved to {save_dir}\")\n",
    "\n",
    "download_model(\"zhbert\", \"hfl/chinese-roberta-wwm-ext\", is_classifier=True)\n",
    "download_model(\"labse\", \"sentence-transformers/LaBSE\", is_classifier=False)\n",
    "download_model(\"cross_encoder\", \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\", is_classifier=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89ab939",
   "metadata": {},
   "source": [
    "## Step 2: Extract Paragraphs from PDF (PyMuPDF + EasyOCR fallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a4e081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import easyocr\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from pdf2image import convert_from_path\n",
    "from pathlib import Path\n",
    "import traceback\n",
    "import glob\n",
    "\n",
    "# Install required poppler-utils if not already installed\n",
    "!apt-get update -qq && apt-get install -qq -y poppler-utils\n",
    "\n",
    "# Base directory settings\n",
    "BASE_DIR = Path(\"/content/NTCIR-18-CLIR-pipeline-team6939\")\n",
    "PDF_DIRS = [\n",
    "    BASE_DIR / \"pdfs/finance\",\n",
    "    BASE_DIR / \"pdfs/insurance\"\n",
    "]\n",
    "OUTPUT_DIR = BASE_DIR / \"outputs\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize EasyOCR\n",
    "try:\n",
    "    print(\"Initializing EasyOCR...\")\n",
    "    reader = easyocr.Reader(['ch_tra', 'en'], gpu=False)\n",
    "    print(\"✓ EasyOCR initialized successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to initialize EasyOCR: {e}\")\n",
    "    print(\"Will try to process without OCR fallback\")\n",
    "    reader = None\n",
    "\n",
    "def extract_blocks_with_heuristics(pdf_path, min_block_length=40):\n",
    "    print(f\"Extracting text blocks using PyMuPDF from: {pdf_path}\")\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        results = []\n",
    "        doc_id = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "\n",
    "        for page_num, page in enumerate(doc):\n",
    "            blocks = page.get_text(\"blocks\")\n",
    "            for i, block in enumerate(sorted(blocks, key=lambda b: b[1])):\n",
    "                x0, y0, x1, y1, text, *_ = block\n",
    "                clean_text = text.strip().replace(\"\\n\", \" \")\n",
    "                if len(clean_text) >= min_block_length:\n",
    "                    results.append({\n",
    "                        \"pid\": f\"{doc_id}_p{page_num}_b{i}\",\n",
    "                        \"page\": page_num,\n",
    "                        \"bbox\": [x0, y0, x1, y1],\n",
    "                        \"text\": clean_text\n",
    "                    })\n",
    "\n",
    "        print(f\"✓ Extracted {len(results)} text blocks\")\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"PyMuPDF extraction failed: {e}\")\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "def fallback_ocr_easyocr(pdf_path):\n",
    "    print(f\"Using EasyOCR for fallback OCR processing: {pdf_path}\")\n",
    "    if reader is None:\n",
    "        print(\"Cannot perform OCR: EasyOCR not initialized\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        images = convert_from_path(pdf_path, dpi=300)\n",
    "        results = []\n",
    "        doc_id = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "\n",
    "        for page_num, image in enumerate(images):\n",
    "            print(f\"Processing page {page_num + 1}...\")\n",
    "\n",
    "            # Convert PIL Image to numpy array (format that EasyOCR expects)\n",
    "            image_np = np.array(image)\n",
    "\n",
    "            # Process with EasyOCR\n",
    "            try:\n",
    "                ocr_result = reader.readtext(image_np)\n",
    "                full_text = \" \".join([res[1] for res in ocr_result if len(res[1].strip()) > 0])\n",
    "\n",
    "                if full_text.strip():\n",
    "                    results.append({\n",
    "                        \"pid\": f\"{doc_id}_ocr_{page_num}\",\n",
    "                        \"page\": page_num,\n",
    "                        \"bbox\": None,\n",
    "                        \"text\": full_text.strip()\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"Error during OCR on page {page_num + 1}: {e}\")\n",
    "                continue\n",
    "\n",
    "        print(f\"✓ OCR processing completed, extracted {len(results)} pages of text\")\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"OCR processing failed: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return []\n",
    "\n",
    "def process_pdf_file(pdf_path):\n",
    "    print(f\"Processing PDF file: {pdf_path}\")\n",
    "\n",
    "    # Check if file exists\n",
    "    if not os.path.exists(pdf_path):\n",
    "        print(f\"Error: PDF file does not exist: {pdf_path}\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        # First try with PyMuPDF\n",
    "        segments = extract_blocks_with_heuristics(pdf_path)\n",
    "\n",
    "        # Skip OCR if PyMuPDF produced good results\n",
    "        if segments and not all(len(seg['text']) < 40 for seg in segments):\n",
    "            return segments\n",
    "\n",
    "        print(f\"PyMuPDF extraction results poor or empty, attempting OCR\")\n",
    "        ocr_segments = fallback_ocr_easyocr(pdf_path)\n",
    "\n",
    "        # If OCR also failed, return whatever we got from PyMuPDF\n",
    "        if not ocr_segments:\n",
    "            print(\"OCR produced no results, returning PyMuPDF results instead\")\n",
    "            return segments\n",
    "\n",
    "        return ocr_segments\n",
    "    except Exception as e:\n",
    "        print(f\"Error during PyMuPDF processing: {e}\")\n",
    "        print(\"Attempting OCR fallback\")\n",
    "\n",
    "        try:\n",
    "            return fallback_ocr_easyocr(pdf_path)\n",
    "        except Exception as e2:\n",
    "            print(f\"OCR fallback also failed: {e2}\")\n",
    "            print(\"Returning empty results for this PDF\")\n",
    "            return []\n",
    "\n",
    "# Find all PDF files in specified directories\n",
    "all_pdf_files = []\n",
    "for pdf_dir in PDF_DIRS:\n",
    "    if pdf_dir.exists():\n",
    "        pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "        all_pdf_files.extend(pdf_files)\n",
    "        print(f\"Found {len(pdf_files)} PDF files in {pdf_dir}\")\n",
    "    else:\n",
    "        print(f\"Warning: Directory does not exist: {pdf_dir}\")\n",
    "\n",
    "print(f\"Total PDFs found: {len(all_pdf_files)}\")\n",
    "\n",
    "# Process a subset of PDFs if there are too many (optional)\n",
    "MAX_PDFS = 99999999999  # Adjust this number as needed\n",
    "if len(all_pdf_files) > MAX_PDFS:\n",
    "    print(f\"Processing first {MAX_PDFS} PDFs out of {len(all_pdf_files)}\")\n",
    "    all_pdf_files = all_pdf_files[:MAX_PDFS]\n",
    "\n",
    "# Process all PDF files\n",
    "all_results = []\n",
    "successful_pdfs = 0\n",
    "for pdf_file in all_pdf_files:\n",
    "    print(f\"\\nProcessing {pdf_file}... ({successful_pdfs+1}/{len(all_pdf_files)})\")\n",
    "    try:\n",
    "        pdf_results = process_pdf_file(pdf_file)\n",
    "        all_results.extend(pdf_results)\n",
    "        print(f\"Extracted {len(pdf_results)} segments from {pdf_file}\")\n",
    "        successful_pdfs += 1\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {pdf_file}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "# Save all results to a single file\n",
    "output_file = OUTPUT_DIR / \"structured_passages.jsonl\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for r in all_results:\n",
    "        json.dump(r, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"✅ Extraction completed, results saved to: {output_file}\")\n",
    "print(f\"Total extracted passages: {len(all_results)}\")\n",
    "print(f\"Successfully processed {successful_pdfs}/{len(all_pdf_files)} PDFs\")\n",
    "\n",
    "# Display some sample text\n",
    "if all_results:\n",
    "    print(\"\\nSample texts:\")\n",
    "    for i, r in enumerate(all_results[:3]):  # Show only first 3\n",
    "        print(f\"[{i+1}] {r['pid']}: {r['text'][:100]}...\")\n",
    "else:\n",
    "    print(\"No text was extracted from any PDF.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cf3bbb",
   "metadata": {},
   "source": [
    "## Step 2.a  PyPDF2 + easyocr fallback （Less granularized on passage chunk handling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0589d0f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPyPDF2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PdfReader\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpdf2image\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m convert_from_path\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01measyocr\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mtraceback\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# 初始化 OCR\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/easyocr/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01measyocr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Reader\n\u001b[32m      3\u001b[39m __version__ = \u001b[33m'\u001b[39m\u001b[33m1.7.2\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/easyocr/easyocr.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrecognition\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_recognizer, get_text\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m group_text_box, get_image_list, calculate_md5, get_paragraph,\\\n\u001b[32m      5\u001b[39m                    download_and_unzip, printProgressBar, diff, reformat_input,\\\n\u001b[32m      6\u001b[39m                    make_rotated_img_list, set_result_with_confidence,\\\n\u001b[32m      7\u001b[39m                    reformat_input_batched, merge_to_free\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/easyocr/recognition.py:2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackends\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcudnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcudnn\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/torch/__init__.py:2611\u001b[39m\n\u001b[32m   2607\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m vmap \u001b[38;5;28;01mas\u001b[39;00m vmap\n\u001b[32m   2610\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m TYPE_CHECKING:\n\u001b[32m-> \u001b[39m\u001b[32m2611\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations\n\u001b[32m   2613\u001b[39m \u001b[38;5;66;03m# Enable CUDA Sanitizer\u001b[39;00m\n\u001b[32m   2614\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mTORCH_CUDA_SANITIZER\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m os.environ:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/torch/_meta_registrations.py:12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_prims_common\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SymBool, SymFloat, Tensor\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_decomp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     13\u001b[39m     _add_op_to_registry,\n\u001b[32m     14\u001b[39m     _convert_out_params,\n\u001b[32m     15\u001b[39m     global_decomposition_table,\n\u001b[32m     16\u001b[39m     meta_table,\n\u001b[32m     17\u001b[39m )\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_ops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpOverload\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_prims\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _prim_elementwise_meta, ELEMENTWISE_PRIM_TYPE_PROMOTION_KIND\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/torch/_decomp/__init__.py:276\u001b[39m\n\u001b[32m    272\u001b[39m             decompositions.pop(op, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    275\u001b[39m \u001b[38;5;66;03m# populate the table\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_decomp\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdecompositions\u001b[39;00m\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_refs\u001b[39;00m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcore_aten_decompositions\u001b[39m() -> \u001b[33m\"\u001b[39m\u001b[33mCustomDecompTable\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/torch/_decomp/decompositions.py:16\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_meta_registrations\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_prims\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mprims\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_prims_common\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mF\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/torch/_prims/__init__.py:525\u001b[39m\n\u001b[32m    517\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[32m    520\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    521\u001b[39m \u001b[38;5;66;03m# Elementwise unary operations\u001b[39;00m\n\u001b[32m    522\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m525\u001b[39m \u001b[38;5;28mabs\u001b[39m = \u001b[43m_make_elementwise_unary_prim\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mabs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimpl_aten\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mabs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtype_promotion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mELEMENTWISE_PRIM_TYPE_PROMOTION_KIND\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCOMPLEX_TO_FLOAT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    530\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    532\u001b[39m acos = _make_elementwise_unary_prim(\n\u001b[32m    533\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33macos\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    534\u001b[39m     impl_aten=torch.acos,\n\u001b[32m    535\u001b[39m     doc=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    536\u001b[39m     type_promotion=ELEMENTWISE_PRIM_TYPE_PROMOTION_KIND.DEFAULT,\n\u001b[32m    537\u001b[39m )\n\u001b[32m    539\u001b[39m acosh = _make_elementwise_unary_prim(\n\u001b[32m    540\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33macosh\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    541\u001b[39m     impl_aten=torch.acosh,\n\u001b[32m    542\u001b[39m     doc=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    543\u001b[39m     type_promotion=ELEMENTWISE_PRIM_TYPE_PROMOTION_KIND.DEFAULT,\n\u001b[32m    544\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/torch/_prims/__init__.py:493\u001b[39m, in \u001b[36m_make_elementwise_unary_prim\u001b[39m\u001b[34m(name, type_promotion, **kwargs)\u001b[39m\n\u001b[32m    486\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_make_elementwise_unary_prim\u001b[39m(\n\u001b[32m    487\u001b[39m     name: \u001b[38;5;28mstr\u001b[39m, *, type_promotion: ELEMENTWISE_PRIM_TYPE_PROMOTION_KIND, **kwargs\n\u001b[32m    488\u001b[39m ):\n\u001b[32m    489\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    490\u001b[39m \u001b[33;03m    Creates an elementwise unary prim.\u001b[39;00m\n\u001b[32m    491\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_make_prim\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m(Tensor self) -> Tensor\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmeta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_prim_elementwise_meta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtype_promotion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtype_promotion\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mRETURN_TYPE\u001b[49m\u001b[43m.\u001b[49m\u001b[43mNEW\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/torch/_prims/__init__.py:321\u001b[39m, in \u001b[36m_make_prim\u001b[39m\u001b[34m(schema, return_type, meta, impl_aten, doc, tags, use_old_custom_ops_api, register_conj_neg_fallthrough)\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    316\u001b[39m     mutates_args = [\n\u001b[32m    317\u001b[39m         arg.name\n\u001b[32m    318\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m cpp_schema.arguments\n\u001b[32m    319\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m arg.alias_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m arg.alias_info.is_write\n\u001b[32m    320\u001b[39m     ]\n\u001b[32m--> \u001b[39m\u001b[32m321\u001b[39m     prim_def = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlibrary\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcustom_op\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprims::\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    323\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_prim_impl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmutates_args\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmutates_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    327\u001b[39m     prim_def.register_fake(meta)\n\u001b[32m    329\u001b[39m     \u001b[38;5;66;03m# all view ops get conj/neg fallthroughs\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/torch/_library/custom_ops.py:173\u001b[39m, in \u001b[36mcustom_op\u001b[39m\u001b[34m(name, fn, mutates_args, device_types, schema)\u001b[39m\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    172\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m inner\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/torch/_library/custom_ops.py:154\u001b[39m, in \u001b[36mcustom_op.<locals>.inner\u001b[39m\u001b[34m(fn)\u001b[39m\n\u001b[32m    151\u001b[39m     schema_str = schema\n\u001b[32m    153\u001b[39m namespace, opname = name.split(\u001b[33m\"\u001b[39m\u001b[33m::\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m result = \u001b[43mCustomOpDef\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    156\u001b[39m     \u001b[38;5;66;03m# Check that schema's alias annotations match those of `mutates_args`.\u001b[39;00m\n\u001b[32m    157\u001b[39m     expected = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/torch/_library/custom_ops.py:204\u001b[39m, in \u001b[36mCustomOpDef.__init__\u001b[39m\u001b[34m(self, namespace, name, schema, fn)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28mself\u001b[39m._autocast_cpu_dtype: Optional[_dtype] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    203\u001b[39m \u001b[38;5;28mself\u001b[39m._lib = get_library_allowing_overwrite(\u001b[38;5;28mself\u001b[39m._namespace, \u001b[38;5;28mself\u001b[39m._name)\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_register_to_dispatcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[38;5;28mself\u001b[39m._disabled_kernel: \u001b[38;5;28mset\u001b[39m = \u001b[38;5;28mset\u001b[39m()\n\u001b[32m    206\u001b[39m OPDEFS[\u001b[38;5;28mself\u001b[39m._qualname] = \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/torch/_library/custom_ops.py:624\u001b[39m, in \u001b[36mCustomOpDef._register_to_dispatcher\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    616\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    617\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThere was no fake impl registered for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    618\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThis is necessary for torch.compile/export/fx tracing to work. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    619\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPlease use `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m._init_fn.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.register_fake` to add an \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    620\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfake impl.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    621\u001b[39m         )\n\u001b[32m    622\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._abstract_fn(*args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m624\u001b[39m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_register_fake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacklevel\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    626\u001b[39m autograd_impl = autograd.make_autograd_impl(\u001b[38;5;28mself\u001b[39m._opoverload, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    627\u001b[39m lib.impl(\u001b[38;5;28mself\u001b[39m._name, autograd_impl, \u001b[33m\"\u001b[39m\u001b[33mAutograd\u001b[39m\u001b[33m\"\u001b[39m, with_keyset=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/torch/library.py:193\u001b[39m, in \u001b[36mLibrary._register_fake\u001b[39m\u001b[34m(self, op_name, fn, _stacklevel)\u001b[39m\n\u001b[32m    190\u001b[39m     _library.utils.warn_deploy()\n\u001b[32m    191\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m source = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_library\u001b[49m\u001b[43m.\u001b[49m\u001b[43mutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_source\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_stacklevel\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    194\u001b[39m frame = sys._getframe(_stacklevel)\n\u001b[32m    195\u001b[39m caller_module = inspect.getmodule(frame)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/torch/_library/utils.py:54\u001b[39m, in \u001b[36mget_source\u001b[39m\u001b[34m(stacklevel)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_source\u001b[39m(stacklevel: \u001b[38;5;28mint\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m     46\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get a string that represents the caller.\u001b[39;00m\n\u001b[32m     47\u001b[39m \n\u001b[32m     48\u001b[39m \u001b[33;03m    Example: \"/path/to/foo.py:42\"\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     52\u001b[39m \u001b[33;03m    etc.\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     frame = \u001b[43minspect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetframeinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_getframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstacklevel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     source = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mframe.filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mframe.lineno\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m source\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/inspect.py:1697\u001b[39m, in \u001b[36mgetframeinfo\u001b[39m\u001b[34m(frame, context)\u001b[39m\n\u001b[32m   1695\u001b[39m start = lineno - \u001b[32m1\u001b[39m - context//\u001b[32m2\u001b[39m\n\u001b[32m   1696\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1697\u001b[39m     lines, lnum = \u001b[43mfindsource\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1698\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[32m   1699\u001b[39m     lines = index = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/inspect.py:1075\u001b[39m, in \u001b[36mfindsource\u001b[39m\u001b[34m(object)\u001b[39m\n\u001b[32m   1072\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (file.startswith(\u001b[33m'\u001b[39m\u001b[33m<\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m file.endswith(\u001b[33m'\u001b[39m\u001b[33m>\u001b[39m\u001b[33m'\u001b[39m)):\n\u001b[32m   1073\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33msource code not available\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1075\u001b[39m module = \u001b[43mgetmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1076\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m module:\n\u001b[32m   1077\u001b[39m     lines = linecache.getlines(file, module.\u001b[34m__dict__\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/inspect.py:1001\u001b[39m, in \u001b[36mgetmodule\u001b[39m\u001b[34m(object, _filename)\u001b[39m\n\u001b[32m    998\u001b[39m         f = getabsfile(module)\n\u001b[32m    999\u001b[39m         \u001b[38;5;66;03m# Always map to the name the module knows itself by\u001b[39;00m\n\u001b[32m   1000\u001b[39m         modulesbyfile[f] = modulesbyfile[\n\u001b[32m-> \u001b[39m\u001b[32m1001\u001b[39m             \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrealpath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m] = module.\u001b[34m__name__\u001b[39m\n\u001b[32m   1002\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m modulesbyfile:\n\u001b[32m   1003\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m sys.modules.get(modulesbyfile[file])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/posixpath.py:415\u001b[39m, in \u001b[36mrealpath\u001b[39m\u001b[34m(filename, strict)\u001b[39m\n\u001b[32m    412\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the canonical path of the specified filename, eliminating any\u001b[39;00m\n\u001b[32m    413\u001b[39m \u001b[33;03msymbolic links encountered in the path.\"\"\"\u001b[39;00m\n\u001b[32m    414\u001b[39m     filename = os.fspath(filename)\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     path, ok = \u001b[43m_joinrealpath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    416\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m abspath(path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/posixpath.py:435\u001b[39m, in \u001b[36m_joinrealpath\u001b[39m\u001b[34m(path, rest, strict, seen)\u001b[39m\n\u001b[32m    432\u001b[39m     path = sep\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m rest:\n\u001b[32m--> \u001b[39m\u001b[32m435\u001b[39m     name, _, rest = rest.partition(sep)\n\u001b[32m    436\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m name \u001b[38;5;129;01mor\u001b[39;00m name == curdir:\n\u001b[32m    437\u001b[39m         \u001b[38;5;66;03m# current dir\u001b[39;00m\n\u001b[32m    438\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 安裝：\n",
    "# pip install PyPDF2 easyocr pdf2image\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "from pdf2image import convert_from_path\n",
    "import easyocr, numpy as np, json, traceback\n",
    "from pathlib import Path\n",
    "\n",
    "# 初始化 OCR\n",
    "reader = easyocr.Reader(['ch_tra','en'], gpu=True)\n",
    "\n",
    "# 資料夾設定\n",
    "PDF_DIRS = [Path(\"./pdfs/finance\"), Path(\"./pdfs/insurance\")]\n",
    "OUTPUT = Path(\"./outputs/structured_passages.jsonl\")\n",
    "OUTPUT.parent.mkdir(exist_ok=True)\n",
    "\n",
    "def extract_text_pypdf2(pdf_path):\n",
    "    try:\n",
    "        rdr = PdfReader(str(pdf_path))\n",
    "        texts = []\n",
    "        for page in rdr.pages:\n",
    "            t = page.extract_text()\n",
    "            if t:\n",
    "                texts.append(t.replace(\"\\n\", \" \"))\n",
    "        return \" \".join(texts)\n",
    "    except Exception as e:\n",
    "        print(f\"PyPDF2 failed on {pdf_path}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return \"\"\n",
    "\n",
    "def fallback_ocr(pdf_path):\n",
    "    try:\n",
    "        imgs = convert_from_path(str(pdf_path), dpi=300)\n",
    "        all_t = []\n",
    "        for img in imgs:\n",
    "            arr = np.array(img)\n",
    "            res = reader.readtext(arr)\n",
    "            txt = \" \".join([r[1] for r in res if r[1].strip()])\n",
    "            if txt:\n",
    "                all_t.append(txt)\n",
    "        return \" \".join(all_t)\n",
    "    except Exception as e:\n",
    "        print(f\"OCR failed on {pdf_path}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return \"\"\n",
    "\n",
    "with open(OUTPUT, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for pdf_dir in PDF_DIRS:\n",
    "        if not pdf_dir.exists(): continue\n",
    "        for pdf in pdf_dir.glob(\"*.pdf\"):\n",
    "            print(f\"\\nProcessing {pdf.name}…\")\n",
    "            text = extract_text_pypdf2(pdf)\n",
    "            source = \"pypdf2\"\n",
    "            if len(text.strip()) < 40:\n",
    "                print(f\" → too little ({len(text)} chars), OCR fallback…\")\n",
    "                text = fallback_ocr(pdf)\n",
    "                source = \"easyocr\"\n",
    "            if text.strip():\n",
    "                rec = {\n",
    "                    \"pid\": pdf.stem,\n",
    "                    \"page\": 0,\n",
    "                    \"bbox\": None,\n",
    "                    \"text\": text.strip(),\n",
    "                    \"source\": source\n",
    "                }\n",
    "                fout.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "                print(f\" ✓ extracted {len(text)} chars via {source}\")\n",
    "            else:\n",
    "                print(f\" ❌ still empty for {pdf.name}\")\n",
    "\n",
    "print(f\"\\nDone! Results in {OUTPUT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93703d3",
   "metadata": {},
   "source": [
    "## Step 3: Hugging Face Translate"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": null,
   "id": "e213303c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🌐 Step 3: Hugging Face MarianMT Translation with Cache\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 下載 HuggingFace 模型 (英文 ➔ 中文)\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-zh\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# 載入 queries\n",
    "with open(\"/content/data/translated_query.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    queries = json.load(f)\n",
    "\n",
    "# 設定 cache 檔案路徑（根據 model 名稱自動命名）\n",
    "cache_path = f\"/content/outputs/translated_cache_{model_name.replace('/', '_')}.json\"\n",
    "if os.path.exists(cache_path):\n",
    "    with open(cache_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        translated_cache = json.load(f)\n",
    "else:\n",
    "    translated_cache = {}\n",
    "\n",
    "# 定義 NMT 翻譯函式（支援 cache）\n",
    "def translate_with_nmt(query_en):\n",
    "    if query_en in translated_cache:\n",
    "        return translated_cache[query_en]\n",
    "    try:\n",
    "        inputs = tokenizer(query_en, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            translated = model.generate(**inputs, max_length=512, num_beams=5)\n",
    "        result = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "        translated_cache[query_en] = result\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Translation error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# 執行批次翻譯\n",
    "translated_output = []\n",
    "for item in tqdm(queries):\n",
    "    zh = translate_with_nmt(item[\"query_en\"])\n",
    "    item[\"query_zh_nmt\"] = zh\n",
    "    translated_output.append(item)\n",
    "\n",
    "# 存檔：翻譯後查詢 + 快取\n",
    "os.makedirs(\"/content/outputs\", exist_ok=True)\n",
    "\n",
    "with open(\"/content/outputs/translated_query_nmt.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(translated_output, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "with open(cache_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(translated_cache, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"✅ HuggingFace NMT 翻譯完成並成功快取保存。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,

   "id": "5ce7643d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Only a single TORCH_LIBRARY can be used to register the namespace prims; please put all of your definitions in a single TORCH_LIBRARY block.  If you were trying to specify implementations, consider using TORCH_LIBRARY_IMPL (which can be duplicated).  If you really intended to define operators for a single namespace in a distributed way, you can use TORCH_LIBRARY_FRAGMENT to explicitly indicate this.  Previous registration of TORCH_LIBRARY was registered at /dev/null:241; latest registration was registered at /dev/null:241",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MarianMTModel, MarianTokenizer\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/transformers/__init__.py:26\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     28\u001b[39m     OptionalDependencyNotAvailable,\n\u001b[32m     29\u001b[39m     _LazyModule,\n\u001b[32m   (...)\u001b[39m\u001b[32m     48\u001b[39m     logging,\n\u001b[32m     49\u001b[39m )\n\u001b[32m     52\u001b[39m logger = logging.get_logger(\u001b[34m__name__\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/transformers/dependency_versions_check.py:16\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdependency_versions_table\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deps\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m require_version, require_version_core\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# order specific notes:\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[32m     25\u001b[39m pkgs_to_check_at_runtime = [\n\u001b[32m     26\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpython\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     27\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtqdm\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     37\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpyyaml\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     38\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/transformers/utils/__init__.py:25\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackbone_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BackboneConfigMixin, BackboneMixin\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat_template_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DocstringParsingException, TypeHintParsingException, get_json_schema\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconstants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdoc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     28\u001b[39m     add_code_sample_docstrings,\n\u001b[32m     29\u001b[39m     add_end_docstrings,\n\u001b[32m   (...)\u001b[39m\u001b[32m     33\u001b[39m     replace_return_docstrings,\n\u001b[32m     34\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/transformers/utils/chat_template_utils.py:40\u001b[39m\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mImage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[32m     43\u001b[39m BASIC_TYPES = (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbool\u001b[39m, Any, \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m), ...)\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# Extracts the initial segment of the docstring, containing the function description\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/torch/__init__.py:2611\u001b[39m\n\u001b[32m   2607\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m vmap \u001b[38;5;28;01mas\u001b[39;00m vmap\n\u001b[32m   2610\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m TYPE_CHECKING:\n\u001b[32m-> \u001b[39m\u001b[32m2611\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations\n\u001b[32m   2613\u001b[39m \u001b[38;5;66;03m# Enable CUDA Sanitizer\u001b[39;00m\n\u001b[32m   2614\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mTORCH_CUDA_SANITIZER\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m os.environ:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/torch/_meta_registrations.py:12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_prims_common\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SymBool, SymFloat, Tensor\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_decomp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     13\u001b[39m     _add_op_to_registry,\n\u001b[32m     14\u001b[39m     _convert_out_params,\n\u001b[32m     15\u001b[39m     global_decomposition_table,\n\u001b[32m     16\u001b[39m     meta_table,\n\u001b[32m     17\u001b[39m )\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_ops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpOverload\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_prims\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _prim_elementwise_meta, ELEMENTWISE_PRIM_TYPE_PROMOTION_KIND\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/torch/_decomp/__init__.py:276\u001b[39m\n\u001b[32m    272\u001b[39m             decompositions.pop(op, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    275\u001b[39m \u001b[38;5;66;03m# populate the table\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_decomp\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdecompositions\u001b[39;00m\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_refs\u001b[39;00m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcore_aten_decompositions\u001b[39m() -> \u001b[33m\"\u001b[39m\u001b[33mCustomDecompTable\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/torch/_decomp/decompositions.py:16\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_meta_registrations\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_prims\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mprims\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_prims_common\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mF\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/torch/_prims/__init__.py:37\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moverrides\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m handle_torch_function, has_torch_function\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_pytree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tree_flatten, tree_map, tree_unflatten\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m prim = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlibrary\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLibrary\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprims\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDEF\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m prim_impl = torch.library.Library(\u001b[33m\"\u001b[39m\u001b[33mprims\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mIMPL\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mCompositeExplicitAutograd\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m prim_backend_select_impl = torch.library.Library(\u001b[33m\"\u001b[39m\u001b[33mprims\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mIMPL\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mBackendSelect\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/torch/library.py:109\u001b[39m, in \u001b[36mLibrary.__init__\u001b[39m\u001b[34m(self, ns, kind, dispatch_key)\u001b[39m\n\u001b[32m    107\u001b[39m frame = traceback.extract_stack(limit=\u001b[32m3\u001b[39m)[\u001b[32m0\u001b[39m]\n\u001b[32m    108\u001b[39m filename, lineno = frame.filename, frame.lineno\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m \u001b[38;5;28mself\u001b[39m.m: Optional[Any] = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_dispatch_library\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdispatch_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlineno\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[38;5;28mself\u001b[39m.ns = ns\n\u001b[32m    113\u001b[39m \u001b[38;5;28mself\u001b[39m._op_defs: \u001b[38;5;28mset\u001b[39m[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[31mRuntimeError\u001b[39m: Only a single TORCH_LIBRARY can be used to register the namespace prims; please put all of your definitions in a single TORCH_LIBRARY block.  If you were trying to specify implementations, consider using TORCH_LIBRARY_IMPL (which can be duplicated).  If you really intended to define operators for a single namespace in a distributed way, you can use TORCH_LIBRARY_FRAGMENT to explicitly indicate this.  Previous registration of TORCH_LIBRARY was registered at /dev/null:241; latest registration was registered at /dev/null:241"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from opencc import OpenCC  # 新增：簡轉繁\n",
    "\n",
    "# 下載 HuggingFace 模型 (英文 ➔ 中文)\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-zh\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# 初始化簡轉繁工具\n",
    "cc = OpenCC('s2t')  # Simplified Chinese ➔ Traditional Chinese\n",
    "\n",
    "# 載入 queries\n",
    "with open(\"/content/NTCIR-18-CLIR-pipeline-team6939/data/translated_query.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    queries = json.load(f)\n",
    "\n",
    "# 設定 cache 檔案路徑（根據 model 名稱自動命名）\n",
    "cache_path = f\"/content/NTCIR-18-CLIR-pipeline-team6939/outputs/translated_cache_{model_name.replace('/', '_')}.json\"\n",
    "if os.path.exists(cache_path):\n",
    "    with open(cache_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        translated_cache = json.load(f)\n",
    "else:\n",
    "    translated_cache = {}\n",
    "\n",
    "# 定義 NMT 翻譯函式（支援 cache + 自訂是否轉繁體）\n",
    "def translate_with_nmt(query_en, convert_to_traditional=True):\n",
    "    if query_en in translated_cache:\n",
    "        return translated_cache[query_en]\n",
    "    try:\n",
    "        inputs = tokenizer(query_en, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            translated = model.generate(**inputs, max_length=512, num_beams=5)\n",
    "        result = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "        if convert_to_traditional:\n",
    "            result = cc.convert(result)  # 自動簡體轉繁體\n",
    "        translated_cache[query_en] = result\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Translation error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# 執行批次翻譯\n",
    "translated_output = []\n",
    "for item in tqdm(queries):\n",
    "    zh = translate_with_nmt(item[\"query_en\"], convert_to_traditional=True)  # 這邊可以控管要不要轉繁體\n",
    "    item[\"query_zh_nmt\"] = zh\n",
    "    translated_output.append(item)\n",
    "\n",
    "# 存檔：翻譯後查詢 + 快取\n",
    "os.makedirs(\"/content/NTCIR-18-CLIR-pipeline-team6939/outputs\", exist_ok=True)\n",
    "\n",
    "with open(\"/content/NTCIR-18-CLIR-pipeline-team6939/outputs/translated_query_nmt.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(translated_output, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "with open(cache_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(translated_cache, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"✅ HuggingFace NMT 簡繁轉換翻譯完成並成功快取保存。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f3a05c",
   "metadata": {},
   "source": [
    "## Step 4: Run Retrieval (4 Models with Runtime Logging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aec4f751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Running BM25 baseline...\n",
      "[⏱️] BM25 baseline took 0.02 seconds.\n",
      "\n",
      "🚀 Running BM25 + Chinese BERT reranker...\n",
      "[⏱️] BM25 + Chinese BERT reranker took 0.02 seconds.\n",
      "\n",
      "🚀 Running Multilingual Dual Encoder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/Resources/Python.app/Contents/MacOS/Python: can't open file '/Users/chiuyiting/Documents/GitHub/NTCIR-18-CLIR-pipeline-team6939/src/retrievers/bm25_only.py': [Errno 2] No such file or directory\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/Resources/Python.app/Contents/MacOS/Python: can't open file '/Users/chiuyiting/Documents/GitHub/NTCIR-18-CLIR-pipeline-team6939/src/reranker/reranker_zhbert.py': [Errno 2] No such file or directory\n",
      "No sentence-transformers model found with name ./models/labse. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Encoding passages...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 27/27 [02:59<00:00,  6.66s/it]\n",
      "Dense retrieval: 100%|██████████| 150/150 [00:07<00:00, 18.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] Dense encoder results saved to outputs/runs/dense_dual_encoder.jsonl\n",
      "[⏱️] Multilingual Dual Encoder took 194.08 seconds.\n",
      "\n",
      "🚀 Running Cross Encoder Reranker...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/transformers/utils/hub.py\", line 424, in cached_files\n",
      "    hf_hub_download(\n",
      "  File \"/Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py\", line 106, in _inner_fn\n",
      "    validate_repo_id(arg_value)\n",
      "  File \"/Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py\", line 154, in validate_repo_id\n",
      "    raise HFValidationError(\n",
      "huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/content/models/cross_encoder'. Use `repo_type` argument if needed.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/chiuyiting/Documents/GitHub/NTCIR-18-CLIR-pipeline-team6939/src/reranker/cross_encoder_multilingual.py\", line 23, in <module>\n",
      "    tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py\", line 946, in from_pretrained\n",
      "    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py\", line 778, in get_tokenizer_config\n",
      "    resolved_config_file = cached_file(\n",
      "                           ^^^^^^^^^^^^\n",
      "  File \"/Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/transformers/utils/hub.py\", line 266, in cached_file\n",
      "    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/transformers/utils/hub.py\", line 470, in cached_files\n",
      "    resolved_files = [\n",
      "                     ^\n",
      "  File \"/Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/transformers/utils/hub.py\", line 471, in <listcomp>\n",
      "    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision) for filename in full_filenames\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/transformers/utils/hub.py\", line 134, in _get_cache_file_to_return\n",
      "    resolved_file = try_to_load_from_cache(path_or_repo_id, full_filename, cache_dir=cache_dir, revision=revision)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py\", line 106, in _inner_fn\n",
      "    validate_repo_id(arg_value)\n",
      "  File \"/Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py\", line 154, in validate_repo_id\n",
      "    raise HFValidationError(\n",
      "huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/content/models/cross_encoder'. Use `repo_type` argument if needed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[⏱️] Cross Encoder Reranker took 2.81 seconds.\n",
      "\n",
      "🧪 Retrieval Runtime Summary:\n",
      "BM25 baseline                           : 0.02 seconds\n",
      "BM25 + Chinese BERT reranker            : 0.02 seconds\n",
      "Multilingual Dual Encoder               : 194.08 seconds\n",
      "Cross Encoder Reranker                  : 2.81 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#%cd ./NTCIR-18-CLIR-pipeline-team6939\n",
    "from run_all_retrievals import run_all_retrievals\n",
    "run_all_retrievals()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc570911",
   "metadata": {},
   "source": [
    "#### (fine-tune done ✅) 目前 zhbert reranker 跑不出來，但因為你說要 fine tune 所以我先跳過他 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad503e56",
   "metadata": {},
   "source": [
    "## Step 5: 合併結果成 retrieval_rankings.json ，然後評分（評分的 code 有大改，因為原本切太細了，等你修好之後 evaluate 也要再改） \n",
    "### 4/29 更：evaluate 改了✅ : MRR 現在多了 cutoff 的參數 （MRR@10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7fdcd8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping bm25_rerank_query_zh_nmt: file not found.\n",
      "⚠️ Skipping bm25_only_query: file not found.\n",
      "⚠️ Skipping bm25_rerank_query: file not found.\n",
      "⚠️ Skipping dense_dual_encoder: file not found.\n",
      "⚠️ Skipping cross_encoder_rerank: file not found.\n",
      "✅ grouped rankings saved to outputs/runs/retrieval_rankings.json\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 55\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# ────────────────────────────────────────────────\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# 4️⃣ 評估所有模型（MRR@10, Recall@10/100, NDCG@10/100）\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# ────────────────────────────────────────────────\u001b[39;00m\n\u001b[32m     54\u001b[39m sys.path.append(\u001b[38;5;28mstr\u001b[39m(BASE / \u001b[33m\"\u001b[39m\u001b[33msrc\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mevaluation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mevaluation_summary\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m evaluate_all_models\n\u001b[32m     57\u001b[39m df = evaluate_all_models(\n\u001b[32m     58\u001b[39m     ranking_path=\u001b[38;5;28mstr\u001b[39m(OUT_RANK),\n\u001b[32m     59\u001b[39m     ground_truth_path=\u001b[38;5;28mstr\u001b[39m(GROUND_TRUTH),\n\u001b[32m     60\u001b[39m     output_csv_path=\u001b[38;5;28mstr\u001b[39m(CSV_OUT),\n\u001b[32m     61\u001b[39m     ks=[\u001b[32m10\u001b[39m, \u001b[32m100\u001b[39m]\n\u001b[32m     62\u001b[39m )\n\u001b[32m     63\u001b[39m df\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NTCIR-18-CLIR-pipeline-team6939/src/evaluation/evaluation_summary.py:2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mevaluation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mevaluation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compute_mrr, compute_recall_at_k, compute_ndcg_at_k\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_json\u001b[39m(path):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# ────────────────────────────────────────────────\n",
    "# 1️⃣ Setup Path\n",
    "# ────────────────────────────────────────────────\n",
    "BASE         = Path(\"./\")  # Colab = 根目錄\n",
    "RUNS_DIR     = BASE / \"outputs\" / \"runs\"\n",
    "OUT_RANK     = RUNS_DIR / \"retrieval_rankings.json\"\n",
    "GROUND_TRUTH = BASE / \"data\" / \"ground_truths_example.json\"\n",
    "CSV_OUT      = BASE / \"outputs\" / \"evaluation_summary.csv\"\n",
    "\n",
    "RUNS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ────────────────────────────────────────────────\n",
    "# 2️⃣ 支援多模型（只匯入已完成的 jsonl）\n",
    "# ────────────────────────────────────────────────\n",
    "retrieval = {}\n",
    "available_models = [\n",
    "    \"bm25_only_query_zh_nmt\",\n",
    "    \"bm25_rerank_query_zh_nmt\",\n",
    "    \"bm25_only_query\",\n",
    "    \"bm25_rerank_query\",\n",
    "    \"dense_dual_encoder\",\n",
    "    \"cross_encoder_rerank\"\n",
    "]\n",
    "\n",
    "for model_name in available_models:\n",
    "    fn = RUNS_DIR / f\"{model_name}.jsonl\"\n",
    "    if not fn.exists():\n",
    "        print(f\"⚠️ Skipping {model_name}: file not found.\")\n",
    "        continue\n",
    "\n",
    "    qid_to_pids = defaultdict(list)\n",
    "    with open(fn, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            r = json.loads(line)\n",
    "            pid = str(r[\"pid\"]).split(\"_\")[0]  # ✨ 只取前半段以對應 ground truth\n",
    "            qid_to_pids[str(r[\"qid\"])].append(pid)\n",
    "    retrieval[model_name] = qid_to_pids\n",
    "\n",
    "# ────────────────────────────────────────────────\n",
    "# 3️⃣ 儲存為 retrieval_rankings.json\n",
    "# ────────────────────────────────────────────────\n",
    "with open(OUT_RANK, 'w', encoding='utf-8') as f:\n",
    "    json.dump(retrieval, f, indent=2, ensure_ascii=False)\n",
    "print(f\"✅ grouped rankings saved to {OUT_RANK}\")\n",
    "\n",
    "# ────────────────────────────────────────────────\n",
    "# 4️⃣ 評估所有模型（MRR@10, Recall@10/100, NDCG@10/100）\n",
    "# ────────────────────────────────────────────────\n",
    "sys.path.append(str(BASE / \"src\"))\n",
    "from evaluation.evaluation_summary import evaluate_all_models\n",
    "\n",
    "df = evaluate_all_models(\n",
    "    ranking_path=str(OUT_RANK),\n",
    "    ground_truth_path=str(GROUND_TRUTH),\n",
    "    output_csv_path=str(CSV_OUT),\n",
    "    ks=[10, 100]\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb3b0f8",
   "metadata": {},
   "source": [
    "                Model  MRR  Recall@10  NDCG@10  Recall@100  NDCG@100\n",
    "0           bm25_only  0.0        0.0      0.0         0.0       0.0\n",
    "1  dense_dual_encoder  0.0        0.0      0.0         0.0       0.0\n",
    "2       cross_encoder  0.0        0.0      0.0         0.0       0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73572e32",
   "metadata": {},
   "source": [
    "## Step 6: Translation Error Impact Analysis (還沒試過)\n",
    "###  4/29 2100更：我也更新了這段 +translate error_analysis的code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "022e41cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== CORRECT_TRANSLATION_HIT (7 samples) ==\n",
      "QID: 39\n",
      "EN: If the insured receives two or more surgeries or specified treatments at the same treatment site within the same policy year, how many times will the company pay the surgical and specified treatment benefit?\n",
      "ZH(NMT): 如果受保人在同一政策年度內在同一治療地點接受兩次或兩次以上的手術或特定治療,公司將支付多少次外科和特定治療福利?\n",
      "PRED_TOPK: ['29', '93', '457', '253', '439']\n",
      "GT: ['29']\n",
      "---\n",
      "\n",
      "== CORRECT_TRANSLATION_MISS (143 samples) ==\n",
      "QID: 1\n",
      "EN: Who bears the relevant fees charged by the remitting bank and intermediary bank?\n",
      "ZH(NMT): 誰承擔匯款銀行和中介銀行收取的有關費用?\n",
      "PRED_TOPK: ['42', '77', '474', '507', '76']\n",
      "GT: ['392']\n",
      "---\n",
      "\n",
      "== WRONG_TRANSLATION_HIT (0 samples) ==\n",
      "\n",
      "== WRONG_TRANSLATION_MISS (0 samples) ==\n"
     ]
    }
   ],
   "source": [
    "from src.analysis.translate_error_analysis import extract_translation_impact\n",
    "\n",
    "impact = extract_translation_impact(\n",
    "    queries_path=\"data/translated_query_nmt.json\",\n",
    "    predictions_path=\"outputs/runs/retrieval_rankings.json\",\n",
    "    ground_truth_path=\"data/ground_truths_example.json\"\n",
    ")\n",
    "\n",
    "for category, group in impact.items():\n",
    "    print(f\"\\n== {category.upper()} ({len(group)} samples) ==\")\n",
    "    for qid, en, zh, pred, gt in group[:1]:  # 每個 category 顯示1個 example\n",
    "        print(f\"QID: {qid}\\nEN: {en}\\nZH(NMT): {zh}\\nPRED_TOPK: {pred}\\nGT: {gt}\\n---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
