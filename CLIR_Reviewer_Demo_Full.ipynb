{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "660916e2",
   "metadata": {},
   "source": [
    "#### DEMO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8953971",
   "metadata": {},
   "source": [
    "## Step 0: Import configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6fa92ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in local environment\n",
      "Project root: /Users/chiuyiting/Documents/GitHub/NTCIR-18-CLIR-pipeline\n"
     ]
    }
   ],
   "source": [
    "# Step 0: Import configuration\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path.cwd()))  # Add current directory to path\n",
    "from config import (\n",
    "    PROJECT_ROOT, ENVIRONMENT, DATA_DIR, MODELS_DIR, OUTPUTS_DIR, PDFS_DIR,\n",
    "    PDFS_FINANCE_DIR, PDFS_INSURANCE_DIR, QUERY_PATH, PASSAGE_PATH,\n",
    "    detect_environment, get_project_root\n",
    ")\n",
    "\n",
    "# Print environment info\n",
    "print(f\"Running in {ENVIRONMENT} environment\")\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f40b9baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'NTCIR-18-CLIR-pipeline'...\n",
      "remote: Enumerating objects: 1869, done.\u001b[K\n",
      "remote: Counting objects: 100% (137/137), done.\u001b[K\n",
      "remote: Compressing objects: 100% (113/113), done.\u001b[K\n",
      "^Cceiving objects:   0% (9/1869), 1.19 MiB | 1.18 MiB/s\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Eric0801/NTCIR-18-CLIR-pipeline.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a0672f",
   "metadata": {},
   "source": [
    "### Git Clone (Make conditional based on environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b313258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository if in Colab/Kaggle\n",
    "if ENVIRONMENT in ['colab', 'kaggle']:\n",
    "    !git clone https://github.com/Eric0801/NTCIR-18-CLIR-pipeline.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df3ae35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /Users/chiuyiting/Library/Python/3.11/lib/python/site-packages (4.47.0)\n",
      "Requirement already satisfied: rank_bm25 in /Users/chiuyiting/Library/Python/3.11/lib/python/site-packages (0.2.2)\n",
      "Requirement already satisfied: sentence-transformers in /Users/chiuyiting/Library/Python/3.11/lib/python/site-packages (4.1.0)\n",
      "Requirement already satisfied: sentencepiece in /Users/chiuyiting/Library/Python/3.11/lib/python/site-packages (0.2.0)\n",
      "Requirement already satisfied: faiss-cpu in /Users/chiuyiting/Library/Python/3.11/lib/python/site-packages (1.11.0)\n",
      "Requirement already satisfied: jieba in /Users/chiuyiting/Library/Python/3.11/lib/python/site-packages (0.42.1)\n",
      "Requirement already satisfied: tqdm in /Users/chiuyiting/Library/Python/3.11/lib/python/site-packages (4.66.4)\n",
      "Requirement already satisfied: opencc in /Users/chiuyiting/Library/Python/3.11/lib/python/site-packages (1.1.9)\n",
      "Requirement already satisfied: pymupdf in /Users/chiuyiting/Library/Python/3.11/lib/python/site-packages (1.25.5)\n",
      "Requirement already satisfied: python-dotenv in /Users/chiuyiting/Library/Python/3.11/lib/python/site-packages (1.0.1)\n",
      "Requirement already satisfied: filelock in /Users/chiuyiting/Library/Python/3.11/lib/python/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /Users/chiuyiting/Library/Python/3.11/lib/python/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/chiuyiting/Library/Python/3.11/lib/python/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/chiuyiting/Library/Python/3.11/lib/python/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/chiuyiting/Library/Python/3.11/lib/python/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/chiuyiting/Library/Python/3.11/lib/python/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /Users/chiuyiting/Library/Python/3.11/lib/python/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/chiuyiting/Library/Python/3.11/lib/python/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/chiuyiting/Library/Python/3.11/lib/python/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/chiuyiting/Library/Python/3.11/lib/python/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/chiuyiting/Library/Python/3.11/lib/python/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/chiuyiting/Library/Python/3.11/lib/python/site-packages (from sentence-transformers) (2.6.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/chiuyiting/Library/Python/3.11/lib/python/site-packages (from sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: scipy in /Users/chiuyiting/Library/Python/3.11/lib/python/site-packages (from sentence-transformers) (1.13.0)\n",
      "Requirement already satisfied: Pillow in /Users/chiuyiting/Library/Python/3.11/lib/python/site-packages (from sentence-transformers) (10.3.0)\n",
      "Requirement already satisfied: networkx in /Users/chiuyiting/Library/Python/3.11/lib/python/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/chiuyiting/Library/Python/3.11/lib/python/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/chiuyiting/Library/Python/3.11/lib/python/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/chiuyiting/Library/Python/3.11/lib/python/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/chiuyiting/Library/Python/3.11/lib/python/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/chiuyiting/Library/Python/3.11/lib/python/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/chiuyiting/Library/Python/3.11/lib/python/site-packages (from scikit-learn->sentence-transformers) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/chiuyiting/Library/Python/3.11/lib/python/site-packages (from scikit-learn->sentence-transformers) (3.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers rank_bm25 sentence-transformers sentencepiece faiss-cpu jieba tqdm opencc pymupdf python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1abff5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in local environment\n",
      "Project root: /Users/chiuyiting/Documents/GitHub/NTCIR-18-CLIR-pipeline\n",
      "Setting up models in local environment...\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "[✓] HuggingFace private token loaded.\n",
      "[✓] zhbert already exists, skipping.\n",
      "[✓] labse already exists, skipping.\n",
      "[✓] cross_encoder already exists, skipping.\n"
     ]
    }
   ],
   "source": [
    "!python3 setup_models.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00332849",
   "metadata": {},
   "source": [
    "## Step 1: Intialize Models\n",
    "\n",
    "if models haven't installed, it will check and install to \"models/\" directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0d24587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] zhbert already exists, skipping download.\n",
      "[✓] labse already exists, skipping download.\n",
      "[✓] cross_encoder already exists, skipping download.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Initialize Models (Colab Local)\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "\n",
    "def download_model(name, hf_id, is_classifier=True):\n",
    "    save_dir = Path(\"models\") / name\n",
    "    if save_dir.exists() and any(save_dir.iterdir()):\n",
    "        print(f\"[✓] {name} already exists, skipping download.\")\n",
    "        return\n",
    "    print(f\"↓ Downloading {name} from HuggingFace...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(hf_id)\n",
    "    model_cls = AutoModelForSequenceClassification if is_classifier else AutoModel\n",
    "    model = model_cls.from_pretrained(hf_id)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    tokenizer.save_pretrained(save_dir)\n",
    "    model.save_pretrained(save_dir)\n",
    "    print(f\"[✓] {name} saved to {save_dir}\")\n",
    "\n",
    "download_model(\"zhbert\", \"hfl/chinese-roberta-wwm-ext\", is_classifier=True)\n",
    "download_model(\"labse\", \"sentence-transformers/LaBSE\", is_classifier=False)\n",
    "download_model(\"cross_encoder\", \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\", is_classifier=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89ab939",
   "metadata": {},
   "source": [
    "## Step 2: Extract Paragraphs from PDF (PyMuPDF + EasyOCR fallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4a4e081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: apt-get\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 30] Read-only file system: '/content'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 21\u001b[0m\n\u001b[1;32m     16\u001b[0m PDF_DIRS \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     17\u001b[0m     BASE_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpdfs/finance\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     18\u001b[0m     BASE_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpdfs/insurance\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     19\u001b[0m ]\n\u001b[1;32m     20\u001b[0m OUTPUT_DIR \u001b[38;5;241m=\u001b[39m BASE_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 21\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOUTPUT_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Initialize EasyOCR\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m<frozen os>:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
      "File \u001b[0;32m<frozen os>:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
      "File \u001b[0;32m<frozen os>:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 30] Read-only file system: '/content'"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "import easyocr\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from pdf2image import convert_from_path\n",
    "from pathlib import Path\n",
    "import traceback\n",
    "import glob\n",
    "\n",
    "# Install required poppler-utils if not already installed\n",
    "!apt-get update -qq && apt-get install -qq -y poppler-utils\n",
    "\n",
    "# Base directory settings\n",
    "BASE_DIR = Path(\"/content/NTCIR-18-CLIR-pipeline-team6939\")\n",
    "PDF_DIRS = [\n",
    "    BASE_DIR / \"pdfs/finance\",\n",
    "    BASE_DIR / \"pdfs/insurance\"\n",
    "]\n",
    "OUTPUT_DIR = BASE_DIR / \"outputs\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize EasyOCR\n",
    "try:\n",
    "    print(\"Initializing EasyOCR...\")\n",
    "    reader = easyocr.Reader(['ch_tra', 'en'], gpu=False)\n",
    "    print(\"✓ EasyOCR initialized successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to initialize EasyOCR: {e}\")\n",
    "    print(\"Will try to process without OCR fallback\")\n",
    "    reader = None\n",
    "\n",
    "def extract_blocks_with_heuristics(pdf_path, min_block_length=40):\n",
    "    print(f\"Extracting text blocks using PyMuPDF from: {pdf_path}\")\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        results = []\n",
    "        doc_id = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "\n",
    "        for page_num, page in enumerate(doc):\n",
    "            blocks = page.get_text(\"blocks\")\n",
    "            for i, block in enumerate(sorted(blocks, key=lambda b: b[1])):\n",
    "                x0, y0, x1, y1, text, *_ = block\n",
    "                clean_text = text.strip().replace(\"\\n\", \" \")\n",
    "                if len(clean_text) >= min_block_length:\n",
    "                    results.append({\n",
    "                        \"pid\": f\"{doc_id}_p{page_num}_b{i}\",\n",
    "                        \"page\": page_num,\n",
    "                        \"bbox\": [x0, y0, x1, y1],\n",
    "                        \"text\": clean_text\n",
    "                    })\n",
    "\n",
    "        print(f\"✓ Extracted {len(results)} text blocks\")\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"PyMuPDF extraction failed: {e}\")\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "def fallback_ocr_easyocr(pdf_path):\n",
    "    print(f\"Using EasyOCR for fallback OCR processing: {pdf_path}\")\n",
    "    if reader is None:\n",
    "        print(\"Cannot perform OCR: EasyOCR not initialized\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        images = convert_from_path(pdf_path, dpi=300)\n",
    "        results = []\n",
    "        doc_id = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "\n",
    "        for page_num, image in enumerate(images):\n",
    "            print(f\"Processing page {page_num + 1}...\")\n",
    "\n",
    "            # Convert PIL Image to numpy array (format that EasyOCR expects)\n",
    "            image_np = np.array(image)\n",
    "\n",
    "            # Process with EasyOCR\n",
    "            try:\n",
    "                ocr_result = reader.readtext(image_np)\n",
    "                full_text = \" \".join([res[1] for res in ocr_result if len(res[1].strip()) > 0])\n",
    "\n",
    "                if full_text.strip():\n",
    "                    results.append({\n",
    "                        \"pid\": f\"{doc_id}_ocr_{page_num}\",\n",
    "                        \"page\": page_num,\n",
    "                        \"bbox\": None,\n",
    "                        \"text\": full_text.strip()\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"Error during OCR on page {page_num + 1}: {e}\")\n",
    "                continue\n",
    "\n",
    "        print(f\"✓ OCR processing completed, extracted {len(results)} pages of text\")\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"OCR processing failed: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return []\n",
    "\n",
    "def process_pdf_file(pdf_path):\n",
    "    print(f\"Processing PDF file: {pdf_path}\")\n",
    "\n",
    "    # Check if file exists\n",
    "    if not os.path.exists(pdf_path):\n",
    "        print(f\"Error: PDF file does not exist: {pdf_path}\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        # First try with PyMuPDF\n",
    "        segments = extract_blocks_with_heuristics(pdf_path)\n",
    "\n",
    "        # Skip OCR if PyMuPDF produced good results\n",
    "        if segments and not all(len(seg['text']) < 40 for seg in segments):\n",
    "            return segments\n",
    "\n",
    "        print(f\"PyMuPDF extraction results poor or empty, attempting OCR\")\n",
    "        ocr_segments = fallback_ocr_easyocr(pdf_path)\n",
    "\n",
    "        # If OCR also failed, return whatever we got from PyMuPDF\n",
    "        if not ocr_segments:\n",
    "            print(\"OCR produced no results, returning PyMuPDF results instead\")\n",
    "            return segments\n",
    "\n",
    "        return ocr_segments\n",
    "    except Exception as e:\n",
    "        print(f\"Error during PyMuPDF processing: {e}\")\n",
    "        print(\"Attempting OCR fallback\")\n",
    "\n",
    "        try:\n",
    "            return fallback_ocr_easyocr(pdf_path)\n",
    "        except Exception as e2:\n",
    "            print(f\"OCR fallback also failed: {e2}\")\n",
    "            print(\"Returning empty results for this PDF\")\n",
    "            return []\n",
    "\n",
    "# Find all PDF files in specified directories\n",
    "all_pdf_files = []\n",
    "for pdf_dir in PDF_DIRS:\n",
    "    if pdf_dir.exists():\n",
    "        pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "        all_pdf_files.extend(pdf_files)\n",
    "        print(f\"Found {len(pdf_files)} PDF files in {pdf_dir}\")\n",
    "    else:\n",
    "        print(f\"Warning: Directory does not exist: {pdf_dir}\")\n",
    "\n",
    "print(f\"Total PDFs found: {len(all_pdf_files)}\")\n",
    "\n",
    "# Process a subset of PDFs if there are too many (optional)\n",
    "MAX_PDFS = 99999999999  # Adjust this number as needed\n",
    "if len(all_pdf_files) > MAX_PDFS:\n",
    "    print(f\"Processing first {MAX_PDFS} PDFs out of {len(all_pdf_files)}\")\n",
    "    all_pdf_files = all_pdf_files[:MAX_PDFS]\n",
    "\n",
    "# Process all PDF files\n",
    "all_results = []\n",
    "successful_pdfs = 0\n",
    "for pdf_file in all_pdf_files:\n",
    "    print(f\"\\nProcessing {pdf_file}... ({successful_pdfs+1}/{len(all_pdf_files)})\")\n",
    "    try:\n",
    "        pdf_results = process_pdf_file(pdf_file)\n",
    "        all_results.extend(pdf_results)\n",
    "        print(f\"Extracted {len(pdf_results)} segments from {pdf_file}\")\n",
    "        successful_pdfs += 1\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {pdf_file}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "# Save all results to a single file\n",
    "output_file = OUTPUT_DIR / \"structured_passages.jsonl\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for r in all_results:\n",
    "        json.dump(r, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"✅ Extraction completed, results saved to: {output_file}\")\n",
    "print(f\"Total extracted passages: {len(all_results)}\")\n",
    "print(f\"Successfully processed {successful_pdfs}/{len(all_pdf_files)} PDFs\")\n",
    "\n",
    "# Display some sample text\n",
    "if all_results:\n",
    "    print(\"\\nSample texts:\")\n",
    "    for i, r in enumerate(all_results[:3]):  # Show only first 3\n",
    "        print(f\"[{i+1}] {r['pid']}: {r['text'][:100]}...\")\n",
    "else:\n",
    "    print(\"No text was extracted from any PDF.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c62bc15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in local environment\n",
      "Project root: /Users/chiuyiting/Documents/GitHub/NTCIR-18-CLIR-pipeline\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nMarianTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m\n",
      "\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# 下載 HuggingFace 模型 (英文 ➔ 中文)\u001b[39;00m\n",
      "\u001b[1;32m     13\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHelsinki-NLP/opus-mt-en-zh\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m---> 14\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mMarianTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(model_name)\n",
      "\u001b[1;32m     15\u001b[0m model \u001b[38;5;241m=\u001b[39m MarianMTModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n",
      "\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# 初始化簡轉繁工具\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/transformers/utils/import_utils.py:1666\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n",
      "\u001b[1;32m   1664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;32m   1665\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n",
      "\u001b[0;32m-> 1666\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/transformers/utils/import_utils.py:1654\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n",
      "\u001b[1;32m   1652\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n",
      "\u001b[1;32m   1653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n",
      "\u001b[0;32m-> 1654\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\n",
      "\u001b[0;31mImportError\u001b[0m: \n",
      "MarianTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from opencc import OpenCC  # 簡轉繁\n",
    "from config import (\n",
    "    QUERY_PATH, OUTPUTS_DIR, get_cache_path,\n",
    "    ensure_dir\n",
    ")\n",
    "\n",
    "# 下載 HuggingFace 模型 (英文 ➔ 中文)\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-zh\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# 初始化簡轉繁工具\n",
    "cc = OpenCC('s2t')  # Simplified Chinese ➔ Traditional Chinese\n",
    "\n",
    "# 載入 queries (使用 config 路徑)\n",
    "with open(QUERY_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    queries = json.load(f)\n",
    "\n",
    "# 設定 cache 檔案路徑（使用 config 函數）\n",
    "cache_path = get_cache_path('nmt')\n",
    "if os.path.exists(cache_path):\n",
    "    with open(cache_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        translated_cache = json.load(f)\n",
    "else:\n",
    "    translated_cache = {}\n",
    "\n",
    "# 定義 NMT 翻譯函式（支援 cache + 自訂是否轉繁體）\n",
    "def translate_with_nmt(query_en, convert_to_traditional=True):\n",
    "    if query_en in translated_cache:\n",
    "        return translated_cache[query_en]\n",
    "    try:\n",
    "        inputs = tokenizer(query_en, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            translated = model.generate(**inputs, max_length=512, num_beams=5)\n",
    "        result = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "        if convert_to_traditional:\n",
    "            result = cc.convert(result)  # 自動簡體轉繁體\n",
    "        translated_cache[query_en] = result\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Translation error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# 執行批次翻譯\n",
    "translated_output = []\n",
    "for item in tqdm(queries):\n",
    "    zh = translate_with_nmt(item[\"query_en\"], convert_to_traditional=True)  # 這邊可以控管要不要轉繁體\n",
    "    item[\"query_zh_nmt\"] = zh\n",
    "    translated_output.append(item)\n",
    "\n",
    "# 確保輸出目錄存在\n",
    "ensure_dir(OUTPUTS_DIR)\n",
    "\n",
    "# 存檔：翻譯後查詢 + 快取（使用 config 路徑）\n",
    "with open(OUTPUTS_DIR / \"translated_query_nmt.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(translated_output, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "with open(cache_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(translated_cache, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"✅ HuggingFace NMT 簡繁轉換翻譯完成並成功快取保存。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cf3bbb",
   "metadata": {},
   "source": [
    "## Step 2.a  PyPDF2 + easyocr fallback （Less granularized on passage chunk handling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0589d0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 570.pdf…\n",
      " ✓ extracted 2441 chars via pypdf2\n",
      "\n",
      "Processing 216.pdf…\n",
      " ✓ extracted 5912 chars via pypdf2\n",
      "\n",
      "Processing 202.pdf…\n",
      " ✓ extracted 653 chars via pypdf2\n",
      "\n",
      "Processing 564.pdf…\n",
      " ✓ extracted 1525 chars via pypdf2\n",
      "\n",
      "Processing 558.pdf…\n",
      " ✓ extracted 1998 chars via pypdf2\n",
      "\n",
      "Processing 1027.pdf…\n",
      " ✓ extracted 870 chars via pypdf2\n",
      "\n",
      "Processing 1033.pdf…\n",
      " ✓ extracted 995 chars via pypdf2\n",
      "\n",
      "Processing 772.pdf…\n",
      " ✓ extracted 2975 chars via pypdf2\n",
      "\n",
      "Processing 766.pdf…\n",
      " ✓ extracted 3604 chars via pypdf2\n",
      "\n",
      "Processing 996.pdf…\n",
      " ✓ extracted 2214 chars via pypdf2\n",
      "\n",
      "Processing 982.pdf…\n",
      " ✓ extracted 812 chars via pypdf2\n",
      "\n",
      "Processing 969.pdf…\n",
      " ✓ extracted 1082 chars via pypdf2\n",
      "\n",
      "Processing 955.pdf…\n",
      " ✓ extracted 1614 chars via pypdf2\n",
      "\n",
      "Processing 941.pdf…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Illegal character in Name Object (b'/ABCDEE+\\xb7s\\xb2\\xd3\\xa9\\xfa\\xc5\\xe9')\n",
      "Illegal character in Name Object (b'/ABCDEE+\\xb7s\\xb2\\xd3\\xa9\\xfa\\xc5\\xe9')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓ extracted 10151 chars via pypdf2\n",
      "\n",
      "Processing 799.pdf…\n",
      " ✓ extracted 613 chars via pypdf2\n",
      "\n",
      "Processing 160.pdf…\n",
      " ✓ extracted 3225 chars via pypdf2\n",
      "\n",
      "Processing 606.pdf…\n",
      " ✓ extracted 61420 chars via pypdf2\n",
      "\n",
      "Processing 612.pdf…\n",
      " ✓ extracted 1917 chars via pypdf2\n",
      "\n",
      "Processing 174.pdf…\n",
      " ✓ extracted 3404 chars via pypdf2\n",
      "\n",
      "Processing 148.pdf…\n",
      " ✓ extracted 5435 chars via pypdf2\n",
      "\n",
      "Processing 49.pdf…\n",
      " ✓ extracted 636 chars via pypdf2\n",
      "\n",
      "Processing 809.pdf…\n",
      " ✓ extracted 2782 chars via pypdf2\n",
      "\n",
      "Processing 61.pdf…\n",
      " ✓ extracted 1565 chars via pypdf2\n",
      "\n",
      "Processing 821.pdf…\n",
      " ✓ extracted 8166 chars via pypdf2\n",
      "\n",
      "Processing 75.pdf…\n",
      " ✓ extracted 2865 chars via pypdf2\n",
      "\n",
      "Processing 835.pdf…\n",
      " ✓ extracted 1034 chars via pypdf2\n",
      "\n",
      "Processing 404.pdf…\n",
      " ✓ extracted 1133 chars via pypdf2\n",
      "\n",
      "Processing 362.pdf…\n",
      " → too little (0 chars), OCR fallback…\n",
      " ✓ extracted 3646 chars via easyocr\n",
      "\n",
      "Processing 376.pdf…\n",
      " ✓ extracted 5061 chars via pypdf2\n",
      "\n",
      "Processing 410.pdf…\n",
      " ✓ extracted 1550 chars via pypdf2\n",
      "\n",
      "Processing 438.pdf…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Illegal character in Name Object (b'/ABCDEE+\\xb7s\\xb2\\xd3\\xa9\\xfa\\xc5\\xe9')\n",
      "Illegal character in Name Object (b'/ABCDEE+\\xb7s\\xb2\\xd3\\xa9\\xfa\\xc5\\xe9')\n",
      "Illegal character in Name Object (b'/ABCDEE+\\xb7s\\xb2\\xd3\\xa9\\xfa\\xc5\\xe9')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓ extracted 71564 chars via pypdf2\n",
      "\n",
      "Processing 389.pdf…\n",
      " ✓ extracted 1103 chars via pypdf2\n",
      "\n",
      "Processing 388.pdf…\n",
      " ✓ extracted 944 chars via pypdf2\n",
      "\n",
      "Processing 439.pdf…\n",
      " ✓ extracted 23545 chars via pypdf2\n",
      "\n",
      "Processing 377.pdf…\n",
      " → too little (0 chars), OCR fallback…\n",
      " ✓ extracted 3300 chars via easyocr\n",
      "\n",
      "Processing 411.pdf…\n",
      " ✓ extracted 3370 chars via pypdf2\n",
      "\n",
      "Processing 405.pdf…\n",
      " ✓ extracted 1342 chars via pypdf2\n",
      "\n",
      "Processing 363.pdf…\n",
      " ✓ extracted 2610 chars via pypdf2\n",
      "\n",
      "Processing 834.pdf…\n",
      " ✓ extracted 2431 chars via pypdf2\n",
      "\n",
      "Processing 74.pdf…\n",
      " ✓ extracted 1139 chars via pypdf2\n",
      "\n",
      "Processing 820.pdf…\n",
      " ✓ extracted 4277 chars via pypdf2\n",
      "\n",
      "Processing 60.pdf…\n",
      " ✓ extracted 7783 chars via pypdf2\n",
      "\n",
      "Processing 808.pdf…\n",
      " ✓ extracted 1900 chars via pypdf2\n",
      "\n",
      "Processing 48.pdf…\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 52\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pdf \u001b[38;5;129;01min\u001b[39;00m pdf_dir\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpdf\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m…\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 52\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mextract_text_pypdf2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     source \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpypdf2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text\u001b[38;5;241m.\u001b[39mstrip()) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m40\u001b[39m:\n",
      "Cell \u001b[0;32mIn[1], line 22\u001b[0m, in \u001b[0;36mextract_text_pypdf2\u001b[0;34m(pdf_path)\u001b[0m\n\u001b[1;32m     20\u001b[0m texts \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m rdr\u001b[38;5;241m.\u001b[39mpages:\n\u001b[0;32m---> 22\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[43mpage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m t:\n\u001b[1;32m     24\u001b[0m         texts\u001b[38;5;241m.\u001b[39mappend(t\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/PyPDF2/_page.py:1851\u001b[0m, in \u001b[0;36mPageObject.extract_text\u001b[0;34m(self, Tj_sep, TJ_sep, orientations, space_width, visitor_operand_before, visitor_operand_after, visitor_text, *args)\u001b[0m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(orientations, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m   1849\u001b[0m     orientations \u001b[38;5;241m=\u001b[39m (orientations,)\n\u001b[0;32m-> 1851\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_text\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1852\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1853\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1854\u001b[0m \u001b[43m    \u001b[49m\u001b[43morientations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspace_width\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mPG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCONTENTS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvisitor_operand_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvisitor_operand_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1859\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvisitor_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/PyPDF2/_page.py:1342\u001b[0m, in \u001b[0;36mPageObject._extract_text\u001b[0;34m(self, obj, pdf, orientations, space_width, content_key, visitor_operand_before, visitor_operand_after, visitor_text)\u001b[0m\n\u001b[1;32m   1340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Font\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resources_dict:\n\u001b[1;32m   1341\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m cast(DictionaryObject, resources_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Font\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[0;32m-> 1342\u001b[0m         cmaps[f] \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_char_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspace_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1343\u001b[0m cmap: Tuple[\n\u001b[1;32m   1344\u001b[0m     Union[\u001b[38;5;28mstr\u001b[39m, Dict[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mstr\u001b[39m]], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mstr\u001b[39m, Optional[DictionaryObject]\n\u001b[1;32m   1345\u001b[0m ] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1350\u001b[0m )  \u001b[38;5;66;03m# (encoding,CMAP,font resource name,dictionary-object of font)\u001b[39;00m\n\u001b[1;32m   1351\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/PyPDF2/_cmap.py:28\u001b[0m, in \u001b[0;36mbuild_char_map\u001b[0;34m(font_name, space_width, obj)\u001b[0m\n\u001b[1;32m     26\u001b[0m space_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[1;32m     27\u001b[0m encoding, space_code \u001b[38;5;241m=\u001b[39m parse_encoding(ft, space_code)\n\u001b[0;32m---> 28\u001b[0m map_dict, space_code, int_entry \u001b[38;5;241m=\u001b[39m \u001b[43mparse_to_unicode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspace_code\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# encoding can be either a string for decode (on 1,2 or a variable number of bytes) of a char table (for 1 byte only for me)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# if empty string, it means it is than encoding field is not present and we have to select the good encoding from cmap input data\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoding \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/PyPDF2/_cmap.py:196\u001b[0m, in \u001b[0;36mparse_to_unicode\u001b[0;34m(ft, space_code)\u001b[0m\n\u001b[1;32m    194\u001b[0m cm \u001b[38;5;241m=\u001b[39m prepare_cm(ft)\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m cm\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 196\u001b[0m     process_rg, process_char, multiline_rg \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_cm_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43ml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_rg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_char\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultiline_rg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint_entry\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a, value \u001b[38;5;129;01min\u001b[39;00m map_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/PyPDF2/_cmap.py:264\u001b[0m, in \u001b[0;36mprocess_cm_line\u001b[0;34m(l, process_rg, process_char, multiline_rg, map_dict, int_entry)\u001b[0m\n\u001b[1;32m    262\u001b[0m     process_char \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m process_rg:\n\u001b[0;32m--> 264\u001b[0m     multiline_rg \u001b[38;5;241m=\u001b[39m \u001b[43mparse_bfrange\u001b[49m\u001b[43m(\u001b[49m\u001b[43ml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint_entry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultiline_rg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m process_char:\n\u001b[1;32m    266\u001b[0m     parse_bfchar(l, map_dict, int_entry)\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/PyPDF2/_cmap.py:276\u001b[0m, in \u001b[0;36mparse_bfrange\u001b[0;34m(l, map_dict, int_entry, multiline_rg)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_bfrange\u001b[39m(\n\u001b[1;32m    271\u001b[0m     l: \u001b[38;5;28mbytes\u001b[39m,\n\u001b[1;32m    272\u001b[0m     map_dict: Dict[Any, Any],\n\u001b[1;32m    273\u001b[0m     int_entry: List[\u001b[38;5;28mint\u001b[39m],\n\u001b[1;32m    274\u001b[0m     multiline_rg: Union[\u001b[38;5;28;01mNone\u001b[39;00m, Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]],\n\u001b[1;32m    275\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[\u001b[38;5;28;01mNone\u001b[39;00m, Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[0;32m--> 276\u001b[0m     lst \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m l\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m x]\n\u001b[1;32m    277\u001b[0m     closure_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    278\u001b[0m     nbi \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(lst[\u001b[38;5;241m0\u001b[39m]), \u001b[38;5;28mlen\u001b[39m(lst[\u001b[38;5;241m1\u001b[39m]))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 安裝：\n",
    "# pip install PyPDF2 easyocr pdf2image\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "from pdf2image import convert_from_path\n",
    "import easyocr, numpy as np, json, traceback\n",
    "from pathlib import Path\n",
    "\n",
    "# 初始化 OCR\n",
    "reader = easyocr.Reader(['ch_tra','en'], gpu=True)\n",
    "\n",
    "# 資料夾設定\n",
    "PDF_DIRS = [Path(\"./pdfs/finance\"), Path(\"./pdfs/insurance\")]\n",
    "OUTPUT = Path(\"./outputs/structured_passages.jsonl\")\n",
    "OUTPUT.parent.mkdir(exist_ok=True)\n",
    "\n",
    "def extract_text_pypdf2(pdf_path):\n",
    "    try:\n",
    "        rdr = PdfReader(str(pdf_path))\n",
    "        texts = []\n",
    "        for page in rdr.pages:\n",
    "            t = page.extract_text()\n",
    "            if t:\n",
    "                texts.append(t.replace(\"\\n\", \" \"))\n",
    "        return \" \".join(texts)\n",
    "    except Exception as e:\n",
    "        print(f\"PyPDF2 failed on {pdf_path}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return \"\"\n",
    "\n",
    "def fallback_ocr(pdf_path):\n",
    "    try:\n",
    "        imgs = convert_from_path(str(pdf_path), dpi=300)\n",
    "        all_t = []\n",
    "        for img in imgs:\n",
    "            arr = np.array(img)\n",
    "            res = reader.readtext(arr)\n",
    "            txt = \" \".join([r[1] for r in res if r[1].strip()])\n",
    "            if txt:\n",
    "                all_t.append(txt)\n",
    "        return \" \".join(all_t)\n",
    "    except Exception as e:\n",
    "        print(f\"OCR failed on {pdf_path}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return \"\"\n",
    "\n",
    "with open(OUTPUT, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for pdf_dir in PDF_DIRS:\n",
    "        if not pdf_dir.exists(): continue\n",
    "        for pdf in pdf_dir.glob(\"*.pdf\"):\n",
    "            print(f\"\\nProcessing {pdf.name}…\")\n",
    "            text = extract_text_pypdf2(pdf)\n",
    "            source = \"pypdf2\"\n",
    "            if len(text.strip()) < 40:\n",
    "                print(f\" → too little ({len(text)} chars), OCR fallback…\")\n",
    "                text = fallback_ocr(pdf)\n",
    "                source = \"easyocr\"\n",
    "            if text.strip():\n",
    "                rec = {\n",
    "                    \"pid\": pdf.stem,\n",
    "                    \"page\": 0,\n",
    "                    \"bbox\": None,\n",
    "                    \"text\": text.strip(),\n",
    "                    \"source\": source\n",
    "                }\n",
    "                fout.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "                print(f\" ✓ extracted {len(text)} chars via {source}\")\n",
    "            else:\n",
    "                print(f\" ❌ still empty for {pdf.name}\")\n",
    "\n",
    "print(f\"\\nDone! Results in {OUTPUT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93703d3",
   "metadata": {},
   "source": [
    "## Step 3: Hugging Face Translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e213303c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nMarianTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# 下載 HuggingFace 模型 (英文 ➔ 中文)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHelsinki-NLP/opus-mt-en-zh\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 14\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mMarianTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(model_name)\n\u001b[1;32m     15\u001b[0m model \u001b[38;5;241m=\u001b[39m MarianMTModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# 初始化簡轉繁工具\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/transformers/utils/import_utils.py:1666\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1665\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1666\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/transformers/utils/import_utils.py:1654\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1652\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[1;32m   1653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[0;32m-> 1654\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[0;31mImportError\u001b[0m: \nMarianTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from opencc import OpenCC  # 簡轉繁\n",
    "from config import (\n",
    "    QUERY_PATH, OUTPUTS_DIR, get_cache_path,\n",
    "    ensure_dir\n",
    ")\n",
    "\n",
    "# 下載 HuggingFace 模型 (英文 ➔ 中文)\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-zh\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# 初始化簡轉繁工具\n",
    "cc = OpenCC('s2t')  # Simplified Chinese ➔ Traditional Chinese\n",
    "\n",
    "# 載入 queries (使用 config 路徑)\n",
    "with open(QUERY_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    queries = json.load(f)\n",
    "\n",
    "# 設定 cache 檔案路徑（使用 config 函數）\n",
    "cache_path = get_cache_path('nmt')\n",
    "if os.path.exists(cache_path):\n",
    "    with open(cache_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        translated_cache = json.load(f)\n",
    "else:\n",
    "    translated_cache = {}\n",
    "\n",
    "# 定義 NMT 翻譯函式（支援 cache + 自訂是否轉繁體）\n",
    "def translate_with_nmt(query_en, convert_to_traditional=True):\n",
    "    if query_en in translated_cache:\n",
    "        return translated_cache[query_en]\n",
    "    try:\n",
    "        inputs = tokenizer(query_en, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            translated = model.generate(**inputs, max_length=512, num_beams=5)\n",
    "        result = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "        if convert_to_traditional:\n",
    "            result = cc.convert(result)  # 自動簡體轉繁體\n",
    "        translated_cache[query_en] = result\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Translation error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# 執行批次翻譯\n",
    "translated_output = []\n",
    "for item in tqdm(queries):\n",
    "    zh = translate_with_nmt(item[\"query_en\"], convert_to_traditional=True)  # 這邊可以控管要不要轉繁體\n",
    "    item[\"query_zh_nmt\"] = zh\n",
    "    translated_output.append(item)\n",
    "\n",
    "# 確保輸出目錄存在\n",
    "ensure_dir(OUTPUTS_DIR)\n",
    "\n",
    "# 存檔：翻譯後查詢 + 快取（使用 config 路徑）\n",
    "with open(OUTPUTS_DIR / \"translated_query_nmt.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(translated_output, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "with open(cache_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(translated_cache, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"✅ HuggingFace NMT 簡繁轉換翻譯完成並成功快取保存。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39219ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in local environment\n",
      "Project root: /Users/chiuyiting/Documents/GitHub/NTCIR-18-CLIR-pipeline\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nMarianTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m\n",
      "\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# 下載 HuggingFace 模型 (英文 ➔ 中文)\u001b[39;00m\n",
      "\u001b[1;32m     13\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHelsinki-NLP/opus-mt-en-zh\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m---> 14\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mMarianTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(model_name)\n",
      "\u001b[1;32m     15\u001b[0m model \u001b[38;5;241m=\u001b[39m MarianMTModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n",
      "\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# 初始化簡轉繁工具\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/transformers/utils/import_utils.py:1666\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n",
      "\u001b[1;32m   1664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;32m   1665\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n",
      "\u001b[0;32m-> 1666\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/transformers/utils/import_utils.py:1654\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n",
      "\u001b[1;32m   1652\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n",
      "\u001b[1;32m   1653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n",
      "\u001b[0;32m-> 1654\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\n",
      "\u001b[0;31mImportError\u001b[0m: \n",
      "MarianTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from opencc import OpenCC  # 簡轉繁\n",
    "from config import (\n",
    "    QUERY_PATH, OUTPUTS_DIR, get_cache_path,\n",
    "    ensure_dir\n",
    ")\n",
    "\n",
    "# 下載 HuggingFace 模型 (英文 ➔ 中文)\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-zh\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# 初始化簡轉繁工具\n",
    "cc = OpenCC('s2t')  # Simplified Chinese ➔ Traditional Chinese\n",
    "\n",
    "# 載入 queries (使用 config 路徑)\n",
    "with open(QUERY_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    queries = json.load(f)\n",
    "\n",
    "# 設定 cache 檔案路徑（使用 config 函數）\n",
    "cache_path = get_cache_path('nmt')\n",
    "if os.path.exists(cache_path):\n",
    "    with open(cache_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        translated_cache = json.load(f)\n",
    "else:\n",
    "    translated_cache = {}\n",
    "\n",
    "# 定義 NMT 翻譯函式（支援 cache + 自訂是否轉繁體）\n",
    "def translate_with_nmt(query_en, convert_to_traditional=True):\n",
    "    if query_en in translated_cache:\n",
    "        return translated_cache[query_en]\n",
    "    try:\n",
    "        inputs = tokenizer(query_en, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            translated = model.generate(**inputs, max_length=512, num_beams=5)\n",
    "        result = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "        if convert_to_traditional:\n",
    "            result = cc.convert(result)  # 自動簡體轉繁體\n",
    "        translated_cache[query_en] = result\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Translation error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# 執行批次翻譯\n",
    "translated_output = []\n",
    "for item in tqdm(queries):\n",
    "    zh = translate_with_nmt(item[\"query_en\"], convert_to_traditional=True)  # 這邊可以控管要不要轉繁體\n",
    "    item[\"query_zh_nmt\"] = zh\n",
    "    translated_output.append(item)\n",
    "\n",
    "# 確保輸出目錄存在\n",
    "ensure_dir(OUTPUTS_DIR)\n",
    "\n",
    "# 存檔：翻譯後查詢 + 快取（使用 config 路徑）\n",
    "with open(OUTPUTS_DIR / \"translated_query_nmt.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(translated_output, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "with open(cache_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(translated_cache, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"✅ HuggingFace NMT 簡繁轉換翻譯完成並成功快取保存。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef14eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in local environment\n",
      "Project root: /Users/chiuyiting/Documents/GitHub/NTCIR-18-CLIR-pipeline\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nMarianTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m\n",
      "\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# 下載 HuggingFace 模型 (英文 ➔ 中文)\u001b[39;00m\n",
      "\u001b[1;32m     13\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHelsinki-NLP/opus-mt-en-zh\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m---> 14\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mMarianTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(model_name)\n",
      "\u001b[1;32m     15\u001b[0m model \u001b[38;5;241m=\u001b[39m MarianMTModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n",
      "\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# 初始化簡轉繁工具\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/transformers/utils/import_utils.py:1666\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n",
      "\u001b[1;32m   1664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;32m   1665\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n",
      "\u001b[0;32m-> 1666\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/transformers/utils/import_utils.py:1654\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n",
      "\u001b[1;32m   1652\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n",
      "\u001b[1;32m   1653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n",
      "\u001b[0;32m-> 1654\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\n",
      "\u001b[0;31mImportError\u001b[0m: \n",
      "MarianTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from opencc import OpenCC  # 簡轉繁\n",
    "from config import (\n",
    "    QUERY_PATH, OUTPUTS_DIR, get_cache_path,\n",
    "    ensure_dir\n",
    ")\n",
    "\n",
    "# 下載 HuggingFace 模型 (英文 ➔ 中文)\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-zh\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# 初始化簡轉繁工具\n",
    "cc = OpenCC('s2t')  # Simplified Chinese ➔ Traditional Chinese\n",
    "\n",
    "# 載入 queries (使用 config 路徑)\n",
    "with open(QUERY_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    queries = json.load(f)\n",
    "\n",
    "# 設定 cache 檔案路徑（使用 config 函數）\n",
    "cache_path = get_cache_path('nmt')\n",
    "if os.path.exists(cache_path):\n",
    "    with open(cache_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        translated_cache = json.load(f)\n",
    "else:\n",
    "    translated_cache = {}\n",
    "\n",
    "# 定義 NMT 翻譯函式（支援 cache + 自訂是否轉繁體）\n",
    "def translate_with_nmt(query_en, convert_to_traditional=True):\n",
    "    if query_en in translated_cache:\n",
    "        return translated_cache[query_en]\n",
    "    try:\n",
    "        inputs = tokenizer(query_en, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            translated = model.generate(**inputs, max_length=512, num_beams=5)\n",
    "        result = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "        if convert_to_traditional:\n",
    "            result = cc.convert(result)  # 自動簡體轉繁體\n",
    "        translated_cache[query_en] = result\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Translation error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# 執行批次翻譯\n",
    "translated_output = []\n",
    "for item in tqdm(queries):\n",
    "    zh = translate_with_nmt(item[\"query_en\"], convert_to_traditional=True)  # 這邊可以控管要不要轉繁體\n",
    "    item[\"query_zh_nmt\"] = zh\n",
    "    translated_output.append(item)\n",
    "\n",
    "# 確保輸出目錄存在\n",
    "ensure_dir(OUTPUTS_DIR)\n",
    "\n",
    "# 存檔：翻譯後查詢 + 快取（使用 config 路徑）\n",
    "with open(OUTPUTS_DIR / \"translated_query_nmt.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(translated_output, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "with open(cache_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(translated_cache, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"✅ HuggingFace NMT 簡繁轉換翻譯完成並成功快取保存。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e427f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in local environment\n",
      "Project root: /Users/chiuyiting/Documents/GitHub/NTCIR-18-CLIR-pipeline\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nMarianTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m\n",
      "\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# 下載 HuggingFace 模型 (英文 ➔ 中文)\u001b[39;00m\n",
      "\u001b[1;32m     13\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHelsinki-NLP/opus-mt-en-zh\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m---> 14\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mMarianTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(model_name)\n",
      "\u001b[1;32m     15\u001b[0m model \u001b[38;5;241m=\u001b[39m MarianMTModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n",
      "\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# 初始化簡轉繁工具\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/transformers/utils/import_utils.py:1666\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n",
      "\u001b[1;32m   1664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;32m   1665\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n",
      "\u001b[0;32m-> 1666\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/transformers/utils/import_utils.py:1654\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n",
      "\u001b[1;32m   1652\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n",
      "\u001b[1;32m   1653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n",
      "\u001b[0;32m-> 1654\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\n",
      "\u001b[0;31mImportError\u001b[0m: \n",
      "MarianTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from opencc import OpenCC  # 簡轉繁\n",
    "from config import (\n",
    "    QUERY_PATH, OUTPUTS_DIR, get_cache_path,\n",
    "    ensure_dir\n",
    ")\n",
    "\n",
    "# 下載 HuggingFace 模型 (英文 ➔ 中文)\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-zh\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# 初始化簡轉繁工具\n",
    "cc = OpenCC('s2t')  # Simplified Chinese ➔ Traditional Chinese\n",
    "\n",
    "# 載入 queries (使用 config 路徑)\n",
    "with open(QUERY_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    queries = json.load(f)\n",
    "\n",
    "# 設定 cache 檔案路徑（使用 config 函數）\n",
    "cache_path = get_cache_path('nmt')\n",
    "if os.path.exists(cache_path):\n",
    "    with open(cache_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        translated_cache = json.load(f)\n",
    "else:\n",
    "    translated_cache = {}\n",
    "\n",
    "# 定義 NMT 翻譯函式（支援 cache + 自訂是否轉繁體）\n",
    "def translate_with_nmt(query_en, convert_to_traditional=True):\n",
    "    if query_en in translated_cache:\n",
    "        return translated_cache[query_en]\n",
    "    try:\n",
    "        inputs = tokenizer(query_en, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            translated = model.generate(**inputs, max_length=512, num_beams=5)\n",
    "        result = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "        if convert_to_traditional:\n",
    "            result = cc.convert(result)  # 自動簡體轉繁體\n",
    "        translated_cache[query_en] = result\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Translation error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# 執行批次翻譯\n",
    "translated_output = []\n",
    "for item in tqdm(queries):\n",
    "    zh = translate_with_nmt(item[\"query_en\"], convert_to_traditional=True)  # 這邊可以控管要不要轉繁體\n",
    "    item[\"query_zh_nmt\"] = zh\n",
    "    translated_output.append(item)\n",
    "\n",
    "# 確保輸出目錄存在\n",
    "ensure_dir(OUTPUTS_DIR)\n",
    "\n",
    "# 存檔：翻譯後查詢 + 快取（使用 config 路徑）\n",
    "with open(OUTPUTS_DIR / \"translated_query_nmt.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(translated_output, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "with open(cache_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(translated_cache, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"✅ HuggingFace NMT 簡繁轉換翻譯完成並成功快取保存。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ce7643d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in local environment\n",
      "Project root: /Users/chiuyiting/Documents/GitHub/NTCIR-18-CLIR-pipeline\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nMarianTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# 下載 HuggingFace 模型 (英文 ➔ 中文)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHelsinki-NLP/opus-mt-en-zh\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 14\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mMarianTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(model_name)\n\u001b[1;32m     15\u001b[0m model \u001b[38;5;241m=\u001b[39m MarianMTModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# 初始化簡轉繁工具\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/transformers/utils/import_utils.py:1666\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1665\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1666\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/transformers/utils/import_utils.py:1654\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1652\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[1;32m   1653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[0;32m-> 1654\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[0;31mImportError\u001b[0m: \nMarianTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from opencc import OpenCC  # 簡轉繁\n",
    "from config import (\n",
    "    QUERY_PATH, OUTPUTS_DIR, get_cache_path,\n",
    "    ensure_dir\n",
    ")\n",
    "\n",
    "# 下載 HuggingFace 模型 (英文 ➔ 中文)\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-zh\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# 初始化簡轉繁工具\n",
    "cc = OpenCC('s2t')  # Simplified Chinese ➔ Traditional Chinese\n",
    "\n",
    "# 載入 queries (使用 config 路徑)\n",
    "with open(QUERY_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    queries = json.load(f)\n",
    "\n",
    "# 設定 cache 檔案路徑（使用 config 函數）\n",
    "cache_path = get_cache_path('nmt')\n",
    "if os.path.exists(cache_path):\n",
    "    with open(cache_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        translated_cache = json.load(f)\n",
    "else:\n",
    "    translated_cache = {}\n",
    "\n",
    "# 定義 NMT 翻譯函式（支援 cache + 自訂是否轉繁體）\n",
    "def translate_with_nmt(query_en, convert_to_traditional=True):\n",
    "    if query_en in translated_cache:\n",
    "        return translated_cache[query_en]\n",
    "    try:\n",
    "        inputs = tokenizer(query_en, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            translated = model.generate(**inputs, max_length=512, num_beams=5)\n",
    "        result = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "        if convert_to_traditional:\n",
    "            result = cc.convert(result)  # 自動簡體轉繁體\n",
    "        translated_cache[query_en] = result\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Translation error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# 執行批次翻譯\n",
    "translated_output = []\n",
    "for item in tqdm(queries):\n",
    "    zh = translate_with_nmt(item[\"query_en\"], convert_to_traditional=True)  # 這邊可以控管要不要轉繁體\n",
    "    item[\"query_zh_nmt\"] = zh\n",
    "    translated_output.append(item)\n",
    "\n",
    "# 確保輸出目錄存在\n",
    "ensure_dir(OUTPUTS_DIR)\n",
    "\n",
    "# 存檔：翻譯後查詢 + 快取（使用 config 路徑）\n",
    "with open(OUTPUTS_DIR / \"translated_query_nmt.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(translated_output, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "with open(cache_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(translated_cache, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"✅ HuggingFace NMT 簡繁轉換翻譯完成並成功快取保存。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee987921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in local environment\n",
      "Project root: /Users/chiuyiting/Documents/GitHub/NTCIR-18-CLIR-pipeline\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nMarianTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m\n",
      "\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# 下載 HuggingFace 模型 (英文 ➔ 中文)\u001b[39;00m\n",
      "\u001b[1;32m     13\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHelsinki-NLP/opus-mt-en-zh\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m---> 14\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mMarianTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(model_name)\n",
      "\u001b[1;32m     15\u001b[0m model \u001b[38;5;241m=\u001b[39m MarianMTModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n",
      "\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# 初始化簡轉繁工具\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/transformers/utils/import_utils.py:1666\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n",
      "\u001b[1;32m   1664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;32m   1665\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n",
      "\u001b[0;32m-> 1666\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/transformers/utils/import_utils.py:1654\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n",
      "\u001b[1;32m   1652\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n",
      "\u001b[1;32m   1653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n",
      "\u001b[0;32m-> 1654\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\n",
      "\u001b[0;31mImportError\u001b[0m: \n",
      "MarianTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from opencc import OpenCC  # 簡轉繁\n",
    "from config import (\n",
    "    QUERY_PATH, OUTPUTS_DIR, get_cache_path,\n",
    "    ensure_dir\n",
    ")\n",
    "\n",
    "# 下載 HuggingFace 模型 (英文 ➔ 中文)\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-zh\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# 初始化簡轉繁工具\n",
    "cc = OpenCC('s2t')  # Simplified Chinese ➔ Traditional Chinese\n",
    "\n",
    "# 載入 queries (使用 config 路徑)\n",
    "with open(QUERY_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    queries = json.load(f)\n",
    "\n",
    "# 設定 cache 檔案路徑（使用 config 函數）\n",
    "cache_path = get_cache_path('nmt')\n",
    "if os.path.exists(cache_path):\n",
    "    with open(cache_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        translated_cache = json.load(f)\n",
    "else:\n",
    "    translated_cache = {}\n",
    "\n",
    "# 定義 NMT 翻譯函式（支援 cache + 自訂是否轉繁體）\n",
    "def translate_with_nmt(query_en, convert_to_traditional=True):\n",
    "    if query_en in translated_cache:\n",
    "        return translated_cache[query_en]\n",
    "    try:\n",
    "        inputs = tokenizer(query_en, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            translated = model.generate(**inputs, max_length=512, num_beams=5)\n",
    "        result = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "        if convert_to_traditional:\n",
    "            result = cc.convert(result)  # 自動簡體轉繁體\n",
    "        translated_cache[query_en] = result\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Translation error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# 執行批次翻譯\n",
    "translated_output = []\n",
    "for item in tqdm(queries):\n",
    "    zh = translate_with_nmt(item[\"query_en\"], convert_to_traditional=True)  # 這邊可以控管要不要轉繁體\n",
    "    item[\"query_zh_nmt\"] = zh\n",
    "    translated_output.append(item)\n",
    "\n",
    "# 確保輸出目錄存在\n",
    "ensure_dir(OUTPUTS_DIR)\n",
    "\n",
    "# 存檔：翻譯後查詢 + 快取（使用 config 路徑）\n",
    "with open(OUTPUTS_DIR / \"translated_query_nmt.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(translated_output, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "with open(cache_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(translated_cache, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"✅ HuggingFace NMT 簡繁轉換翻譯完成並成功快取保存。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f20427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in local environment\n",
      "Project root: /Users/chiuyiting/Documents/GitHub/NTCIR-18-CLIR-pipeline\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nMarianTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m\n",
      "\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# 下載 HuggingFace 模型 (英文 ➔ 中文)\u001b[39;00m\n",
      "\u001b[1;32m     13\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHelsinki-NLP/opus-mt-en-zh\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m---> 14\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mMarianTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(model_name)\n",
      "\u001b[1;32m     15\u001b[0m model \u001b[38;5;241m=\u001b[39m MarianMTModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n",
      "\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# 初始化簡轉繁工具\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/transformers/utils/import_utils.py:1666\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n",
      "\u001b[1;32m   1664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;32m   1665\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n",
      "\u001b[0;32m-> 1666\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/transformers/utils/import_utils.py:1654\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n",
      "\u001b[1;32m   1652\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n",
      "\u001b[1;32m   1653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n",
      "\u001b[0;32m-> 1654\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\n",
      "\u001b[0;31mImportError\u001b[0m: \n",
      "MarianTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from opencc import OpenCC  # 簡轉繁\n",
    "from config import (\n",
    "    QUERY_PATH, OUTPUTS_DIR, get_cache_path,\n",
    "    ensure_dir\n",
    ")\n",
    "\n",
    "# 下載 HuggingFace 模型 (英文 ➔ 中文)\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-zh\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# 初始化簡轉繁工具\n",
    "cc = OpenCC('s2t')  # Simplified Chinese ➔ Traditional Chinese\n",
    "\n",
    "# 載入 queries (使用 config 路徑)\n",
    "with open(QUERY_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    queries = json.load(f)\n",
    "\n",
    "# 設定 cache 檔案路徑（使用 config 函數）\n",
    "cache_path = get_cache_path('nmt')\n",
    "if os.path.exists(cache_path):\n",
    "    with open(cache_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        translated_cache = json.load(f)\n",
    "else:\n",
    "    translated_cache = {}\n",
    "\n",
    "# 定義 NMT 翻譯函式（支援 cache + 自訂是否轉繁體）\n",
    "def translate_with_nmt(query_en, convert_to_traditional=True):\n",
    "    if query_en in translated_cache:\n",
    "        return translated_cache[query_en]\n",
    "    try:\n",
    "        inputs = tokenizer(query_en, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            translated = model.generate(**inputs, max_length=512, num_beams=5)\n",
    "        result = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "        if convert_to_traditional:\n",
    "            result = cc.convert(result)  # 自動簡體轉繁體\n",
    "        translated_cache[query_en] = result\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Translation error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# 執行批次翻譯\n",
    "translated_output = []\n",
    "for item in tqdm(queries):\n",
    "    zh = translate_with_nmt(item[\"query_en\"], convert_to_traditional=True)  # 這邊可以控管要不要轉繁體\n",
    "    item[\"query_zh_nmt\"] = zh\n",
    "    translated_output.append(item)\n",
    "\n",
    "# 確保輸出目錄存在\n",
    "ensure_dir(OUTPUTS_DIR)\n",
    "\n",
    "# 存檔：翻譯後查詢 + 快取（使用 config 路徑）\n",
    "with open(OUTPUTS_DIR / \"translated_query_nmt.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(translated_output, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "with open(cache_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(translated_cache, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"✅ HuggingFace NMT 簡繁轉換翻譯完成並成功快取保存。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f880eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in local environment\n",
      "Project root: /Users/chiuyiting/Documents/GitHub/NTCIR-18-CLIR-pipeline\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nMarianTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m\n",
      "\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# 下載 HuggingFace 模型 (英文 ➔ 中文)\u001b[39;00m\n",
      "\u001b[1;32m     13\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHelsinki-NLP/opus-mt-en-zh\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m---> 14\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mMarianTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(model_name)\n",
      "\u001b[1;32m     15\u001b[0m model \u001b[38;5;241m=\u001b[39m MarianMTModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n",
      "\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# 初始化簡轉繁工具\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/transformers/utils/import_utils.py:1666\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n",
      "\u001b[1;32m   1664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;32m   1665\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n",
      "\u001b[0;32m-> 1666\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/transformers/utils/import_utils.py:1654\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n",
      "\u001b[1;32m   1652\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n",
      "\u001b[1;32m   1653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n",
      "\u001b[0;32m-> 1654\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\n",
      "\u001b[0;31mImportError\u001b[0m: \n",
      "MarianTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from opencc import OpenCC  # 簡轉繁\n",
    "from config import (\n",
    "    QUERY_PATH, OUTPUTS_DIR, get_cache_path,\n",
    "    ensure_dir\n",
    ")\n",
    "\n",
    "# 下載 HuggingFace 模型 (英文 ➔ 中文)\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-zh\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# 初始化簡轉繁工具\n",
    "cc = OpenCC('s2t')  # Simplified Chinese ➔ Traditional Chinese\n",
    "\n",
    "# 載入 queries (使用 config 路徑)\n",
    "with open(QUERY_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    queries = json.load(f)\n",
    "\n",
    "# 設定 cache 檔案路徑（使用 config 函數）\n",
    "cache_path = get_cache_path('nmt')\n",
    "if os.path.exists(cache_path):\n",
    "    with open(cache_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        translated_cache = json.load(f)\n",
    "else:\n",
    "    translated_cache = {}\n",
    "\n",
    "# 定義 NMT 翻譯函式（支援 cache + 自訂是否轉繁體）\n",
    "def translate_with_nmt(query_en, convert_to_traditional=True):\n",
    "    if query_en in translated_cache:\n",
    "        return translated_cache[query_en]\n",
    "    try:\n",
    "        inputs = tokenizer(query_en, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            translated = model.generate(**inputs, max_length=512, num_beams=5)\n",
    "        result = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "        if convert_to_traditional:\n",
    "            result = cc.convert(result)  # 自動簡體轉繁體\n",
    "        translated_cache[query_en] = result\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Translation error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# 執行批次翻譯\n",
    "translated_output = []\n",
    "for item in tqdm(queries):\n",
    "    zh = translate_with_nmt(item[\"query_en\"], convert_to_traditional=True)  # 這邊可以控管要不要轉繁體\n",
    "    item[\"query_zh_nmt\"] = zh\n",
    "    translated_output.append(item)\n",
    "\n",
    "# 確保輸出目錄存在\n",
    "ensure_dir(OUTPUTS_DIR)\n",
    "\n",
    "# 存檔：翻譯後查詢 + 快取（使用 config 路徑）\n",
    "with open(OUTPUTS_DIR / \"translated_query_nmt.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(translated_output, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "with open(cache_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(translated_cache, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"✅ HuggingFace NMT 簡繁轉換翻譯完成並成功快取保存。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71396598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in local environment\n",
      "Project root: /Users/chiuyiting/Documents/GitHub/NTCIR-18-CLIR-pipeline\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nMarianTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m\n",
      "\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# 下載 HuggingFace 模型 (英文 ➔ 中文)\u001b[39;00m\n",
      "\u001b[1;32m     13\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHelsinki-NLP/opus-mt-en-zh\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m---> 14\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mMarianTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(model_name)\n",
      "\u001b[1;32m     15\u001b[0m model \u001b[38;5;241m=\u001b[39m MarianMTModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n",
      "\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# 初始化簡轉繁工具\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/transformers/utils/import_utils.py:1666\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n",
      "\u001b[1;32m   1664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;32m   1665\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n",
      "\u001b[0;32m-> 1666\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/transformers/utils/import_utils.py:1654\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n",
      "\u001b[1;32m   1652\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n",
      "\u001b[1;32m   1653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n",
      "\u001b[0;32m-> 1654\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\n",
      "\u001b[0;31mImportError\u001b[0m: \n",
      "MarianTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from opencc import OpenCC  # 簡轉繁\n",
    "from config import (\n",
    "    QUERY_PATH, OUTPUTS_DIR, get_cache_path,\n",
    "    ensure_dir\n",
    ")\n",
    "\n",
    "# 下載 HuggingFace 模型 (英文 ➔ 中文)\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-zh\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# 初始化簡轉繁工具\n",
    "cc = OpenCC('s2t')  # Simplified Chinese ➔ Traditional Chinese\n",
    "\n",
    "# 載入 queries (使用 config 路徑)\n",
    "with open(QUERY_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    queries = json.load(f)\n",
    "\n",
    "# 設定 cache 檔案路徑（使用 config 函數）\n",
    "cache_path = get_cache_path('nmt')\n",
    "if os.path.exists(cache_path):\n",
    "    with open(cache_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        translated_cache = json.load(f)\n",
    "else:\n",
    "    translated_cache = {}\n",
    "\n",
    "# 定義 NMT 翻譯函式（支援 cache + 自訂是否轉繁體）\n",
    "def translate_with_nmt(query_en, convert_to_traditional=True):\n",
    "    if query_en in translated_cache:\n",
    "        return translated_cache[query_en]\n",
    "    try:\n",
    "        inputs = tokenizer(query_en, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            translated = model.generate(**inputs, max_length=512, num_beams=5)\n",
    "        result = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "        if convert_to_traditional:\n",
    "            result = cc.convert(result)  # 自動簡體轉繁體\n",
    "        translated_cache[query_en] = result\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Translation error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# 執行批次翻譯\n",
    "translated_output = []\n",
    "for item in tqdm(queries):\n",
    "    zh = translate_with_nmt(item[\"query_en\"], convert_to_traditional=True)  # 這邊可以控管要不要轉繁體\n",
    "    item[\"query_zh_nmt\"] = zh\n",
    "    translated_output.append(item)\n",
    "\n",
    "# 確保輸出目錄存在\n",
    "ensure_dir(OUTPUTS_DIR)\n",
    "\n",
    "# 存檔：翻譯後查詢 + 快取（使用 config 路徑）\n",
    "with open(OUTPUTS_DIR / \"translated_query_nmt.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(translated_output, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "with open(cache_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(translated_cache, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"✅ HuggingFace NMT 簡繁轉換翻譯完成並成功快取保存。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22875c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in local environment\n",
      "Project root: /Users/chiuyiting/Documents/GitHub/NTCIR-18-CLIR-pipeline\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nMarianTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m\n",
      "\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# 下載 HuggingFace 模型 (英文 ➔ 中文)\u001b[39;00m\n",
      "\u001b[1;32m     13\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHelsinki-NLP/opus-mt-en-zh\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m---> 14\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mMarianTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(model_name)\n",
      "\u001b[1;32m     15\u001b[0m model \u001b[38;5;241m=\u001b[39m MarianMTModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n",
      "\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# 初始化簡轉繁工具\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/transformers/utils/import_utils.py:1666\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n",
      "\u001b[1;32m   1664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;32m   1665\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n",
      "\u001b[0;32m-> 1666\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/transformers/utils/import_utils.py:1654\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n",
      "\u001b[1;32m   1652\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n",
      "\u001b[1;32m   1653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n",
      "\u001b[0;32m-> 1654\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\n",
      "\u001b[0;31mImportError\u001b[0m: \n",
      "MarianTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from opencc import OpenCC  # 簡轉繁\n",
    "from config import (\n",
    "    QUERY_PATH, OUTPUTS_DIR, get_cache_path,\n",
    "    ensure_dir\n",
    ")\n",
    "\n",
    "# 下載 HuggingFace 模型 (英文 ➔ 中文)\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-zh\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# 初始化簡轉繁工具\n",
    "cc = OpenCC('s2t')  # Simplified Chinese ➔ Traditional Chinese\n",
    "\n",
    "# 載入 queries (使用 config 路徑)\n",
    "with open(QUERY_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    queries = json.load(f)\n",
    "\n",
    "# 設定 cache 檔案路徑（使用 config 函數）\n",
    "cache_path = get_cache_path('nmt')\n",
    "if os.path.exists(cache_path):\n",
    "    with open(cache_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        translated_cache = json.load(f)\n",
    "else:\n",
    "    translated_cache = {}\n",
    "\n",
    "# 定義 NMT 翻譯函式（支援 cache + 自訂是否轉繁體）\n",
    "def translate_with_nmt(query_en, convert_to_traditional=True):\n",
    "    if query_en in translated_cache:\n",
    "        return translated_cache[query_en]\n",
    "    try:\n",
    "        inputs = tokenizer(query_en, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            translated = model.generate(**inputs, max_length=512, num_beams=5)\n",
    "        result = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "        if convert_to_traditional:\n",
    "            result = cc.convert(result)  # 自動簡體轉繁體\n",
    "        translated_cache[query_en] = result\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Translation error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# 執行批次翻譯\n",
    "translated_output = []\n",
    "for item in tqdm(queries):\n",
    "    zh = translate_with_nmt(item[\"query_en\"], convert_to_traditional=True)  # 這邊可以控管要不要轉繁體\n",
    "    item[\"query_zh_nmt\"] = zh\n",
    "    translated_output.append(item)\n",
    "\n",
    "# 確保輸出目錄存在\n",
    "ensure_dir(OUTPUTS_DIR)\n",
    "\n",
    "# 存檔：翻譯後查詢 + 快取（使用 config 路徑）\n",
    "with open(OUTPUTS_DIR / \"translated_query_nmt.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(translated_output, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "with open(cache_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(translated_cache, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"✅ HuggingFace NMT 簡繁轉換翻譯完成並成功快取保存。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebd19d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from opencc import OpenCC  # 簡轉繁\n",
    "from config import (\n",
    "    QUERY_PATH, OUTPUTS_DIR, get_cache_path,\n",
    "    ensure_dir\n",
    ")\n",
    "\n",
    "# 下載 HuggingFace 模型 (英文 ➔ 中文)\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-zh\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# 初始化簡轉繁工具\n",
    "cc = OpenCC('s2t')  # Simplified Chinese ➔ Traditional Chinese\n",
    "\n",
    "# 載入 queries (使用 config 路徑)\n",
    "with open(QUERY_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    queries = json.load(f)\n",
    "\n",
    "# 設定 cache 檔案路徑（使用 config 函數）\n",
    "cache_path = get_cache_path('nmt')\n",
    "if os.path.exists(cache_path):\n",
    "    with open(cache_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        translated_cache = json.load(f)\n",
    "else:\n",
    "    translated_cache = {}\n",
    "\n",
    "# 定義 NMT 翻譯函式（支援 cache + 自訂是否轉繁體）\n",
    "def translate_with_nmt(query_en, convert_to_traditional=True):\n",
    "    if query_en in translated_cache:\n",
    "        return translated_cache[query_en]\n",
    "    try:\n",
    "        inputs = tokenizer(query_en, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            translated = model.generate(**inputs, max_length=512, num_beams=5)\n",
    "        result = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "        if convert_to_traditional:\n",
    "            result = cc.convert(result)  # 自動簡體轉繁體\n",
    "        translated_cache[query_en] = result\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Translation error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# 執行批次翻譯\n",
    "translated_output = []\n",
    "for item in tqdm(queries):\n",
    "    zh = translate_with_nmt(item[\"query_en\"], convert_to_traditional=True)  # 這邊可以控管要不要轉繁體\n",
    "    item[\"query_zh_nmt\"] = zh\n",
    "    translated_output.append(item)\n",
    "\n",
    "# 確保輸出目錄存在\n",
    "ensure_dir(OUTPUTS_DIR)\n",
    "\n",
    "# 存檔：翻譯後查詢 + 快取（使用 config 路徑）\n",
    "with open(OUTPUTS_DIR / \"translated_query_nmt.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(translated_output, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "with open(cache_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(translated_cache, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"✅ HuggingFace NMT 簡繁轉換翻譯完成並成功快取保存。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cecd4ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from opencc import OpenCC  # 簡轉繁\n",
    "from config import (\n",
    "    QUERY_PATH, OUTPUTS_DIR, get_cache_path,\n",
    "    ensure_dir\n",
    ")\n",
    "\n",
    "# 下載 HuggingFace 模型 (英文 ➔ 中文)\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-zh\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# 初始化簡轉繁工具\n",
    "cc = OpenCC('s2t')  # Simplified Chinese ➔ Traditional Chinese\n",
    "\n",
    "# 載入 queries (使用 config 路徑)\n",
    "with open(QUERY_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    queries = json.load(f)\n",
    "\n",
    "# 設定 cache 檔案路徑（使用 config 函數）\n",
    "cache_path = get_cache_path('nmt')\n",
    "if os.path.exists(cache_path):\n",
    "    with open(cache_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        translated_cache = json.load(f)\n",
    "else:\n",
    "    translated_cache = {}\n",
    "\n",
    "# 定義 NMT 翻譯函式（支援 cache + 自訂是否轉繁體）\n",
    "def translate_with_nmt(query_en, convert_to_traditional=True):\n",
    "    if query_en in translated_cache:\n",
    "        return translated_cache[query_en]\n",
    "    try:\n",
    "        inputs = tokenizer(query_en, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            translated = model.generate(**inputs, max_length=512, num_beams=5)\n",
    "        result = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "        if convert_to_traditional:\n",
    "            result = cc.convert(result)  # 自動簡體轉繁體\n",
    "        translated_cache[query_en] = result\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Translation error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# 執行批次翻譯\n",
    "translated_output = []\n",
    "for item in tqdm(queries):\n",
    "    zh = translate_with_nmt(item[\"query_en\"], convert_to_traditional=True)  # 這邊可以控管要不要轉繁體\n",
    "    item[\"query_zh_nmt\"] = zh\n",
    "    translated_output.append(item)\n",
    "\n",
    "# 確保輸出目錄存在\n",
    "ensure_dir(OUTPUTS_DIR)\n",
    "\n",
    "# 存檔：翻譯後查詢 + 快取（使用 config 路徑）\n",
    "with open(OUTPUTS_DIR / \"translated_query_nmt.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(translated_output, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "with open(cache_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(translated_cache, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"✅ HuggingFace NMT 簡繁轉換翻譯完成並成功快取保存。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10212df",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from opencc import OpenCC  # 簡轉繁\n",
    "from config import (\n",
    "    QUERY_PATH, OUTPUTS_DIR, get_cache_path,\n",
    "    ensure_dir\n",
    ")\n",
    "\n",
    "# 下載 HuggingFace 模型 (英文 ➔ 中文)\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-zh\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# 初始化簡轉繁工具\n",
    "cc = OpenCC('s2t')  # Simplified Chinese ➔ Traditional Chinese\n",
    "\n",
    "# 載入 queries (使用 config 路徑)\n",
    "with open(QUERY_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    queries = json.load(f)\n",
    "\n",
    "# 設定 cache 檔案路徑（使用 config 函數）\n",
    "cache_path = get_cache_path('nmt')\n",
    "if os.path.exists(cache_path):\n",
    "    with open(cache_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        translated_cache = json.load(f)\n",
    "else:\n",
    "    translated_cache = {}\n",
    "\n",
    "# 定義 NMT 翻譯函式（支援 cache + 自訂是否轉繁體）\n",
    "def translate_with_nmt(query_en, convert_to_traditional=True):\n",
    "    if query_en in translated_cache:\n",
    "        return translated_cache[query_en]\n",
    "    try:\n",
    "        inputs = tokenizer(query_en, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            translated = model.generate(**inputs, max_length=512, num_beams=5)\n",
    "        result = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "        if convert_to_traditional:\n",
    "            result = cc.convert(result)  # 自動簡體轉繁體\n",
    "        translated_cache[query_en] = result\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Translation error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# 執行批次翻譯\n",
    "translated_output = []\n",
    "for item in tqdm(queries):\n",
    "    zh = translate_with_nmt(item[\"query_en\"], convert_to_traditional=True)  # 這邊可以控管要不要轉繁體\n",
    "    item[\"query_zh_nmt\"] = zh\n",
    "    translated_output.append(item)\n",
    "\n",
    "# 確保輸出目錄存在\n",
    "ensure_dir(OUTPUTS_DIR)\n",
    "\n",
    "# 存檔：翻譯後查詢 + 快取（使用 config 路徑）\n",
    "with open(OUTPUTS_DIR / \"translated_query_nmt.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(translated_output, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "with open(cache_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(translated_cache, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"✅ HuggingFace NMT 簡繁轉換翻譯完成並成功快取保存。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f3a05c",
   "metadata": {},
   "source": [
    "## Step 4: Run Retrieval (4 Models with Runtime Logging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aec4f751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Running BM25 baseline...\n",
      "[⏱️] BM25 baseline took 0.02 seconds.\n",
      "\n",
      "🚀 Running BM25 + Chinese BERT reranker...\n",
      "[⏱️] BM25 + Chinese BERT reranker took 0.02 seconds.\n",
      "\n",
      "🚀 Running Multilingual Dual Encoder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/Resources/Python.app/Contents/MacOS/Python: can't open file '/Users/chiuyiting/Documents/GitHub/NTCIR-18-CLIR-pipeline-team6939/src/retrievers/bm25_only.py': [Errno 2] No such file or directory\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/Resources/Python.app/Contents/MacOS/Python: can't open file '/Users/chiuyiting/Documents/GitHub/NTCIR-18-CLIR-pipeline-team6939/src/reranker/reranker_zhbert.py': [Errno 2] No such file or directory\n",
      "No sentence-transformers model found with name ./models/labse. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Encoding passages...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 27/27 [02:59<00:00,  6.66s/it]\n",
      "Dense retrieval: 100%|██████████| 150/150 [00:07<00:00, 18.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] Dense encoder results saved to outputs/runs/dense_dual_encoder.jsonl\n",
      "[⏱️] Multilingual Dual Encoder took 194.08 seconds.\n",
      "\n",
      "🚀 Running Cross Encoder Reranker...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/transformers/utils/hub.py\", line 424, in cached_files\n",
      "    hf_hub_download(\n",
      "  File \"/Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py\", line 106, in _inner_fn\n",
      "    validate_repo_id(arg_value)\n",
      "  File \"/Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py\", line 154, in validate_repo_id\n",
      "    raise HFValidationError(\n",
      "huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/content/models/cross_encoder'. Use `repo_type` argument if needed.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/chiuyiting/Documents/GitHub/NTCIR-18-CLIR-pipeline-team6939/src/reranker/cross_encoder_multilingual.py\", line 23, in <module>\n",
      "    tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py\", line 946, in from_pretrained\n",
      "    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py\", line 778, in get_tokenizer_config\n",
      "    resolved_config_file = cached_file(\n",
      "                           ^^^^^^^^^^^^\n",
      "  File \"/Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/transformers/utils/hub.py\", line 266, in cached_file\n",
      "    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/transformers/utils/hub.py\", line 470, in cached_files\n",
      "    resolved_files = [\n",
      "                     ^\n",
      "  File \"/Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/transformers/utils/hub.py\", line 471, in <listcomp>\n",
      "    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision) for filename in full_filenames\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/transformers/utils/hub.py\", line 134, in _get_cache_file_to_return\n",
      "    resolved_file = try_to_load_from_cache(path_or_repo_id, full_filename, cache_dir=cache_dir, revision=revision)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py\", line 106, in _inner_fn\n",
      "    validate_repo_id(arg_value)\n",
      "  File \"/Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py\", line 154, in validate_repo_id\n",
      "    raise HFValidationError(\n",
      "huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/content/models/cross_encoder'. Use `repo_type` argument if needed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[⏱️] Cross Encoder Reranker took 2.81 seconds.\n",
      "\n",
      "🧪 Retrieval Runtime Summary:\n",
      "BM25 baseline                           : 0.02 seconds\n",
      "BM25 + Chinese BERT reranker            : 0.02 seconds\n",
      "Multilingual Dual Encoder               : 194.08 seconds\n",
      "Cross Encoder Reranker                  : 2.81 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#%cd ./NTCIR-18-CLIR-pipeline-team6939\n",
    "from run_all_retrievals import run_all_retrievals\n",
    "run_all_retrievals()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc570911",
   "metadata": {},
   "source": [
    "#### (fine-tune done ✅) 目前 zhbert reranker 跑不出來，但因為你說要 fine tune 所以我先跳過他 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad503e56",
   "metadata": {},
   "source": [
    "## Step 5: 合併結果成 retrieval_rankings.json ，然後評分（評分的 code 有大改，因為原本切太細了，等你修好之後 evaluate 也要再改） \n",
    "### 4/29 更：evaluate 改了✅ : MRR 現在多了 cutoff 的參數 （MRR@10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7fdcd8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping bm25_rerank_query_zh_nmt: file not found.\n",
      "⚠️ Skipping bm25_only_query: file not found.\n",
      "⚠️ Skipping bm25_rerank_query: file not found.\n",
      "⚠️ Skipping dense_dual_encoder: file not found.\n",
      "⚠️ Skipping cross_encoder_rerank: file not found.\n",
      "✅ grouped rankings saved to outputs/runs/retrieval_rankings.json\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 55\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# ────────────────────────────────────────────────\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# 4️⃣ 評估所有模型（MRR@10, Recall@10/100, NDCG@10/100）\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# ────────────────────────────────────────────────\u001b[39;00m\n\u001b[32m     54\u001b[39m sys.path.append(\u001b[38;5;28mstr\u001b[39m(BASE / \u001b[33m\"\u001b[39m\u001b[33msrc\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mevaluation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mevaluation_summary\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m evaluate_all_models\n\u001b[32m     57\u001b[39m df = evaluate_all_models(\n\u001b[32m     58\u001b[39m     ranking_path=\u001b[38;5;28mstr\u001b[39m(OUT_RANK),\n\u001b[32m     59\u001b[39m     ground_truth_path=\u001b[38;5;28mstr\u001b[39m(GROUND_TRUTH),\n\u001b[32m     60\u001b[39m     output_csv_path=\u001b[38;5;28mstr\u001b[39m(CSV_OUT),\n\u001b[32m     61\u001b[39m     ks=[\u001b[32m10\u001b[39m, \u001b[32m100\u001b[39m]\n\u001b[32m     62\u001b[39m )\n\u001b[32m     63\u001b[39m df\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NTCIR-18-CLIR-pipeline-team6939/src/evaluation/evaluation_summary.py:2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mevaluation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mevaluation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compute_mrr, compute_recall_at_k, compute_ndcg_at_k\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_json\u001b[39m(path):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# ────────────────────────────────────────────────\n",
    "# 1️⃣ Setup Path\n",
    "# ────────────────────────────────────────────────\n",
    "BASE         = Path(\"./\")  # Colab = 根目錄\n",
    "RUNS_DIR     = BASE / \"outputs\" / \"runs\"\n",
    "OUT_RANK     = RUNS_DIR / \"retrieval_rankings.json\"\n",
    "GROUND_TRUTH = BASE / \"data\" / \"ground_truths_example.json\"\n",
    "CSV_OUT      = BASE / \"outputs\" / \"evaluation_summary.csv\"\n",
    "\n",
    "RUNS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ────────────────────────────────────────────────\n",
    "# 2️⃣ 支援多模型（只匯入已完成的 jsonl）\n",
    "# ────────────────────────────────────────────────\n",
    "retrieval = {}\n",
    "available_models = [\n",
    "    \"bm25_only_query_zh_nmt\",\n",
    "    \"bm25_rerank_query_zh_nmt\",\n",
    "    \"bm25_only_query\",\n",
    "    \"bm25_rerank_query\",\n",
    "    \"dense_dual_encoder\",\n",
    "    \"cross_encoder_rerank\"\n",
    "]\n",
    "\n",
    "for model_name in available_models:\n",
    "    fn = RUNS_DIR / f\"{model_name}.jsonl\"\n",
    "    if not fn.exists():\n",
    "        print(f\"⚠️ Skipping {model_name}: file not found.\")\n",
    "        continue\n",
    "\n",
    "    qid_to_pids = defaultdict(list)\n",
    "    with open(fn, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            r = json.loads(line)\n",
    "            pid = str(r[\"pid\"]).split(\"_\")[0]  # ✨ 只取前半段以對應 ground truth\n",
    "            qid_to_pids[str(r[\"qid\"])].append(pid)\n",
    "    retrieval[model_name] = qid_to_pids\n",
    "\n",
    "# ────────────────────────────────────────────────\n",
    "# 3️⃣ 儲存為 retrieval_rankings.json\n",
    "# ────────────────────────────────────────────────\n",
    "with open(OUT_RANK, 'w', encoding='utf-8') as f:\n",
    "    json.dump(retrieval, f, indent=2, ensure_ascii=False)\n",
    "print(f\"✅ grouped rankings saved to {OUT_RANK}\")\n",
    "\n",
    "# ────────────────────────────────────────────────\n",
    "# 4️⃣ 評估所有模型（MRR@10, Recall@10/100, NDCG@10/100）\n",
    "# ────────────────────────────────────────────────\n",
    "sys.path.append(str(BASE / \"src\"))\n",
    "from evaluation.evaluation_summary import evaluate_all_models\n",
    "\n",
    "df = evaluate_all_models(\n",
    "    ranking_path=str(OUT_RANK),\n",
    "    ground_truth_path=str(GROUND_TRUTH),\n",
    "    output_csv_path=str(CSV_OUT),\n",
    "    ks=[10, 100]\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb3b0f8",
   "metadata": {},
   "source": [
    "                Model  MRR  Recall@10  NDCG@10  Recall@100  NDCG@100\n",
    "0           bm25_only  0.0        0.0      0.0         0.0       0.0\n",
    "1  dense_dual_encoder  0.0        0.0      0.0         0.0       0.0\n",
    "2       cross_encoder  0.0        0.0      0.0         0.0       0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73572e32",
   "metadata": {},
   "source": [
    "## Step 6: Translation Error Impact Analysis (還沒試過)\n",
    "###  4/29 2100更：我也更新了這段 +translate error_analysis的code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "022e41cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== CORRECT_TRANSLATION_HIT (7 samples) ==\n",
      "QID: 39\n",
      "EN: If the insured receives two or more surgeries or specified treatments at the same treatment site within the same policy year, how many times will the company pay the surgical and specified treatment benefit?\n",
      "ZH(NMT): 如果受保人在同一政策年度內在同一治療地點接受兩次或兩次以上的手術或特定治療,公司將支付多少次外科和特定治療福利?\n",
      "PRED_TOPK: ['29', '93', '457', '253', '439']\n",
      "GT: ['29']\n",
      "---\n",
      "\n",
      "== CORRECT_TRANSLATION_MISS (143 samples) ==\n",
      "QID: 1\n",
      "EN: Who bears the relevant fees charged by the remitting bank and intermediary bank?\n",
      "ZH(NMT): 誰承擔匯款銀行和中介銀行收取的有關費用?\n",
      "PRED_TOPK: ['42', '77', '474', '507', '76']\n",
      "GT: ['392']\n",
      "---\n",
      "\n",
      "== WRONG_TRANSLATION_HIT (0 samples) ==\n",
      "\n",
      "== WRONG_TRANSLATION_MISS (0 samples) ==\n"
     ]
    }
   ],
   "source": [
    "from src.analysis.translate_error_analysis import extract_translation_impact\n",
    "\n",
    "impact = extract_translation_impact(\n",
    "    queries_path=\"data/translated_query_nmt.json\",\n",
    "    predictions_path=\"outputs/runs/retrieval_rankings.json\",\n",
    "    ground_truth_path=\"data/ground_truths_example.json\"\n",
    ")\n",
    "\n",
    "for category, group in impact.items():\n",
    "    print(f\"\\n== {category.upper()} ({len(group)} samples) ==\")\n",
    "    for qid, en, zh, pred, gt in group[:1]:  # 每個 category 顯示1個 example\n",
    "        print(f\"QID: {qid}\\nEN: {en}\\nZH(NMT): {zh}\\nPRED_TOPK: {pred}\\nGT: {gt}\\n---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
