{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "660916e2",
   "metadata": {},
   "source": [
    "#### DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b313258",
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/Eric0801/NTCIR-18-CLIR-pipeline-team6939.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df3ae35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting rank_bm25\n",
      "  Using cached rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.11.0-cp311-cp311-macosx_14_0_arm64.whl.metadata (4.8 kB)\n",
      "Collecting jieba\n",
      "  Using cached jieba-0.42.1-py3-none-any.whl\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting opencc\n",
      "  Downloading opencc-1.1.9.tar.gz (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pymupdf in /Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages (1.25.5)\n",
      "Requirement already satisfied: filelock in /Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages (from transformers) (3.18.0)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages (from transformers) (2.2.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting requests (from transformers)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages (from sentence-transformers) (2.7.0)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Downloading scikit_learn-1.6.1-cp311-cp311-macosx_12_0_arm64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: scipy in /Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: Pillow in /Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages (from sentence-transformers) (11.2.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->transformers)\n",
      "  Downloading charset_normalizer-3.4.1-cp311-cp311-macosx_10_9_universal2.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
      "  Downloading urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
      "  Downloading certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence-transformers)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
      "Downloading sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n",
      "Downloading faiss_cpu-1.11.0-cp311-cp311-macosx_14_0_arm64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading regex-2024.11.6-cp311-cp311-macosx_11_0_arm64.whl (284 kB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl (418 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading charset_normalizer-3.4.1-cp311-cp311-macosx_10_9_universal2.whl (194 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Downloading certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
      "Downloading scikit_learn-1.6.1-cp311-cp311-macosx_12_0_arm64.whl (11.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Building wheels for collected packages: opencc\n",
      "  Building wheel for opencc (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for opencc: filename=opencc-1.1.9-cp311-cp311-macosx_11_0_arm64.whl size=1399536 sha256=29792e6cecd65e25dac69b546565c01f37738b632e53710b5ffb9a28a65b6759\n",
      "  Stored in directory: /Users/chiuyiting/Library/Caches/pip/wheels/a8/69/af/d8229790f384e1e6ca80d0459a9754679d44e554711f909d8f\n",
      "Successfully built opencc\n",
      "Installing collected packages: opencc, jieba, urllib3, tqdm, threadpoolctl, safetensors, regex, rank_bm25, joblib, idna, faiss-cpu, charset-normalizer, certifi, scikit-learn, requests, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19/19\u001b[0m [sentence-transformers]ence-transformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed certifi-2025.4.26 charset-normalizer-3.4.1 faiss-cpu-1.11.0 huggingface-hub-0.30.2 idna-3.10 jieba-0.42.1 joblib-1.4.2 opencc-1.1.9 rank_bm25-0.2.2 regex-2024.11.6 requests-2.32.3 safetensors-0.5.3 scikit-learn-1.6.1 sentence-transformers-4.1.0 threadpoolctl-3.6.0 tokenizers-0.21.1 tqdm-4.67.1 transformers-4.51.3 urllib3-2.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install transformers rank_bm25 sentence-transformers faiss-cpu jieba tqdm opencc pymupdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1abff5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "↓ Downloading zhbert from hfl/chinese-roberta-wwm-ext...\n",
      "tokenizer_config.json: 100%|█████████████████| 19.0/19.0 [00:00<00:00, 23.0kB/s]\n",
      "config.json: 100%|█████████████████████████████| 689/689 [00:00<00:00, 4.18MB/s]\n",
      "vocab.txt: 100%|██████████████████████████████| 110k/110k [00:00<00:00, 259kB/s]\n",
      "tokenizer.json: 100%|█████████████████████████| 269k/269k [00:00<00:00, 773kB/s]\n",
      "added_tokens.json: 100%|█████████████████████| 2.00/2.00 [00:00<00:00, 9.22kB/s]\n",
      "special_tokens_map.json: 100%|██████████████████| 112/112 [00:00<00:00, 773kB/s]\n",
      "pytorch_model.bin: 100%|█████████████████████| 412M/412M [01:35<00:00, 4.33MB/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-roberta-wwm-ext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[✓] Saved to models/zhbert\n",
      "↓ Downloading labse from sentence-transformers/LaBSE...\n",
      "tokenizer_config.json: 100%|███████████████████| 397/397 [00:00<00:00, 5.95MB/s]\n",
      "config.json: 100%|█████████████████████████████| 804/804 [00:00<00:00, 12.0MB/s]\n",
      "vocab.txt: 100%|███████████████████████████| 5.22M/5.22M [00:01<00:00, 3.83MB/s]\n",
      "tokenizer.json: 100%|██████████████████████| 9.62M/9.62M [00:02<00:00, 3.92MB/s]\n",
      "special_tokens_map.json: 100%|██████████████████| 112/112 [00:00<00:00, 605kB/s]\n",
      "model.safetensors: 100%|███████████████████| 1.88G/1.88G [09:04<00:00, 3.46MB/s]\n",
      "[✓] Saved to models/labse\n",
      "↓ Downloading cross_encoder from sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2...\n",
      "tokenizer_config.json: 100%|███████████████████| 480/480 [00:00<00:00, 4.95MB/s]\n",
      "config.json: 100%|█████████████████████████████| 645/645 [00:00<00:00, 1.88MB/s]\n",
      "tokenizer.json: 100%|██████████████████████| 9.08M/9.08M [00:03<00:00, 2.58MB/s]\n",
      "special_tokens_map.json: 100%|█████████████████| 239/239 [00:00<00:00, 1.25MB/s]\n",
      "model.safetensors: 100%|█████████████████████| 471M/471M [02:39<00:00, 2.96MB/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[✓] Saved to models/cross_encoder\n"
     ]
    }
   ],
   "source": [
    "!python3 setup_models.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00332849",
   "metadata": {},
   "source": [
    "## Step 1: Intialize Models\n",
    "\n",
    "if models haven't installed, it will check and install to \"models/\" directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0d24587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] zhbert already exists, skipping download.\n",
      "[✓] labse already exists, skipping download.\n",
      "[✓] cross_encoder already exists, skipping download.\n"
     ]
    }
   ],
   "source": [
    "# Step 0: Initialize Models (Colab Local)\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "\n",
    "def download_model(name, hf_id, is_classifier=True):\n",
    "    save_dir = Path(\"models\") / name\n",
    "    if save_dir.exists() and any(save_dir.iterdir()):\n",
    "        print(f\"[✓] {name} already exists, skipping download.\")\n",
    "        return\n",
    "    print(f\"↓ Downloading {name} from HuggingFace...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(hf_id)\n",
    "    model_cls = AutoModelForSequenceClassification if is_classifier else AutoModel\n",
    "    model = model_cls.from_pretrained(hf_id)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    tokenizer.save_pretrained(save_dir)\n",
    "    model.save_pretrained(save_dir)\n",
    "    print(f\"[✓] {name} saved to {save_dir}\")\n",
    "\n",
    "download_model(\"zhbert\", \"hfl/chinese-roberta-wwm-ext\", is_classifier=True)\n",
    "download_model(\"labse\", \"sentence-transformers/LaBSE\", is_classifier=False)\n",
    "download_model(\"cross_encoder\", \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\", is_classifier=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89ab939",
   "metadata": {},
   "source": [
    "## Step 2: Extract Paragraphs from PDF (PyMuPDF + EasyOCR fallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a4e081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import easyocr\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from pdf2image import convert_from_path\n",
    "from pathlib import Path\n",
    "import traceback\n",
    "import glob\n",
    "\n",
    "# Install required poppler-utils if not already installed\n",
    "!apt-get update -qq && apt-get install -qq -y poppler-utils\n",
    "\n",
    "# Base directory settings\n",
    "BASE_DIR = Path(\"/content/NTCIR-18-CLIR-pipeline-team6939\")\n",
    "PDF_DIRS = [\n",
    "    BASE_DIR / \"pdfs/finance\",\n",
    "    BASE_DIR / \"pdfs/insurance\"\n",
    "]\n",
    "OUTPUT_DIR = BASE_DIR / \"outputs\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize EasyOCR\n",
    "try:\n",
    "    print(\"Initializing EasyOCR...\")\n",
    "    reader = easyocr.Reader(['ch_tra', 'en'], gpu=False)\n",
    "    print(\"✓ EasyOCR initialized successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to initialize EasyOCR: {e}\")\n",
    "    print(\"Will try to process without OCR fallback\")\n",
    "    reader = None\n",
    "\n",
    "def extract_blocks_with_heuristics(pdf_path, min_block_length=40):\n",
    "    print(f\"Extracting text blocks using PyMuPDF from: {pdf_path}\")\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        results = []\n",
    "        doc_id = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "\n",
    "        for page_num, page in enumerate(doc):\n",
    "            blocks = page.get_text(\"blocks\")\n",
    "            for i, block in enumerate(sorted(blocks, key=lambda b: b[1])):\n",
    "                x0, y0, x1, y1, text, *_ = block\n",
    "                clean_text = text.strip().replace(\"\\n\", \" \")\n",
    "                if len(clean_text) >= min_block_length:\n",
    "                    results.append({\n",
    "                        \"pid\": f\"{doc_id}_p{page_num}_b{i}\",\n",
    "                        \"page\": page_num,\n",
    "                        \"bbox\": [x0, y0, x1, y1],\n",
    "                        \"text\": clean_text\n",
    "                    })\n",
    "\n",
    "        print(f\"✓ Extracted {len(results)} text blocks\")\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"PyMuPDF extraction failed: {e}\")\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "def fallback_ocr_easyocr(pdf_path):\n",
    "    print(f\"Using EasyOCR for fallback OCR processing: {pdf_path}\")\n",
    "    if reader is None:\n",
    "        print(\"Cannot perform OCR: EasyOCR not initialized\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        images = convert_from_path(pdf_path, dpi=300)\n",
    "        results = []\n",
    "        doc_id = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "\n",
    "        for page_num, image in enumerate(images):\n",
    "            print(f\"Processing page {page_num + 1}...\")\n",
    "\n",
    "            # Convert PIL Image to numpy array (format that EasyOCR expects)\n",
    "            image_np = np.array(image)\n",
    "\n",
    "            # Process with EasyOCR\n",
    "            try:\n",
    "                ocr_result = reader.readtext(image_np)\n",
    "                full_text = \" \".join([res[1] for res in ocr_result if len(res[1].strip()) > 0])\n",
    "\n",
    "                if full_text.strip():\n",
    "                    results.append({\n",
    "                        \"pid\": f\"{doc_id}_ocr_{page_num}\",\n",
    "                        \"page\": page_num,\n",
    "                        \"bbox\": None,\n",
    "                        \"text\": full_text.strip()\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"Error during OCR on page {page_num + 1}: {e}\")\n",
    "                continue\n",
    "\n",
    "        print(f\"✓ OCR processing completed, extracted {len(results)} pages of text\")\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"OCR processing failed: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return []\n",
    "\n",
    "def process_pdf_file(pdf_path):\n",
    "    print(f\"Processing PDF file: {pdf_path}\")\n",
    "\n",
    "    # Check if file exists\n",
    "    if not os.path.exists(pdf_path):\n",
    "        print(f\"Error: PDF file does not exist: {pdf_path}\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        # First try with PyMuPDF\n",
    "        segments = extract_blocks_with_heuristics(pdf_path)\n",
    "\n",
    "        # Skip OCR if PyMuPDF produced good results\n",
    "        if segments and not all(len(seg['text']) < 40 for seg in segments):\n",
    "            return segments\n",
    "\n",
    "        print(f\"PyMuPDF extraction results poor or empty, attempting OCR\")\n",
    "        ocr_segments = fallback_ocr_easyocr(pdf_path)\n",
    "\n",
    "        # If OCR also failed, return whatever we got from PyMuPDF\n",
    "        if not ocr_segments:\n",
    "            print(\"OCR produced no results, returning PyMuPDF results instead\")\n",
    "            return segments\n",
    "\n",
    "        return ocr_segments\n",
    "    except Exception as e:\n",
    "        print(f\"Error during PyMuPDF processing: {e}\")\n",
    "        print(\"Attempting OCR fallback\")\n",
    "\n",
    "        try:\n",
    "            return fallback_ocr_easyocr(pdf_path)\n",
    "        except Exception as e2:\n",
    "            print(f\"OCR fallback also failed: {e2}\")\n",
    "            print(\"Returning empty results for this PDF\")\n",
    "            return []\n",
    "\n",
    "# Find all PDF files in specified directories\n",
    "all_pdf_files = []\n",
    "for pdf_dir in PDF_DIRS:\n",
    "    if pdf_dir.exists():\n",
    "        pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "        all_pdf_files.extend(pdf_files)\n",
    "        print(f\"Found {len(pdf_files)} PDF files in {pdf_dir}\")\n",
    "    else:\n",
    "        print(f\"Warning: Directory does not exist: {pdf_dir}\")\n",
    "\n",
    "print(f\"Total PDFs found: {len(all_pdf_files)}\")\n",
    "\n",
    "# Process a subset of PDFs if there are too many (optional)\n",
    "MAX_PDFS = 99999999999  # Adjust this number as needed\n",
    "if len(all_pdf_files) > MAX_PDFS:\n",
    "    print(f\"Processing first {MAX_PDFS} PDFs out of {len(all_pdf_files)}\")\n",
    "    all_pdf_files = all_pdf_files[:MAX_PDFS]\n",
    "\n",
    "# Process all PDF files\n",
    "all_results = []\n",
    "successful_pdfs = 0\n",
    "for pdf_file in all_pdf_files:\n",
    "    print(f\"\\nProcessing {pdf_file}... ({successful_pdfs+1}/{len(all_pdf_files)})\")\n",
    "    try:\n",
    "        pdf_results = process_pdf_file(pdf_file)\n",
    "        all_results.extend(pdf_results)\n",
    "        print(f\"Extracted {len(pdf_results)} segments from {pdf_file}\")\n",
    "        successful_pdfs += 1\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {pdf_file}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "# Save all results to a single file\n",
    "output_file = OUTPUT_DIR / \"structured_passages.jsonl\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for r in all_results:\n",
    "        json.dump(r, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"✅ Extraction completed, results saved to: {output_file}\")\n",
    "print(f\"Total extracted passages: {len(all_results)}\")\n",
    "print(f\"Successfully processed {successful_pdfs}/{len(all_pdf_files)} PDFs\")\n",
    "\n",
    "# Display some sample text\n",
    "if all_results:\n",
    "    print(\"\\nSample texts:\")\n",
    "    for i, r in enumerate(all_results[:3]):  # Show only first 3\n",
    "        print(f\"[{i+1}] {r['pid']}: {r['text'][:100]}...\")\n",
    "else:\n",
    "    print(\"No text was extracted from any PDF.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cf3bbb",
   "metadata": {},
   "source": [
    "## Step 2.a  PyPDF2 + easyocr fallback （Less granularized on passage chunk handling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0589d0f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPyPDF2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PdfReader\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpdf2image\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m convert_from_path\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01measyocr\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mtraceback\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# 初始化 OCR\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/easyocr/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01measyocr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Reader\n\u001b[32m      3\u001b[39m __version__ = \u001b[33m'\u001b[39m\u001b[33m1.7.2\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/easyocr/easyocr.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrecognition\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_recognizer, get_text\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m group_text_box, get_image_list, calculate_md5, get_paragraph,\\\n\u001b[32m      5\u001b[39m                    download_and_unzip, printProgressBar, diff, reformat_input,\\\n\u001b[32m      6\u001b[39m                    make_rotated_img_list, set_result_with_confidence,\\\n\u001b[32m      7\u001b[39m                    reformat_input_batched, merge_to_free\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/easyocr/recognition.py:2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackends\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcudnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcudnn\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/torch/__init__.py:2611\u001b[39m\n\u001b[32m   2607\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m vmap \u001b[38;5;28;01mas\u001b[39;00m vmap\n\u001b[32m   2610\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m TYPE_CHECKING:\n\u001b[32m-> \u001b[39m\u001b[32m2611\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations\n\u001b[32m   2613\u001b[39m \u001b[38;5;66;03m# Enable CUDA Sanitizer\u001b[39;00m\n\u001b[32m   2614\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mTORCH_CUDA_SANITIZER\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m os.environ:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/torch/_meta_registrations.py:12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_prims_common\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SymBool, SymFloat, Tensor\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_decomp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     13\u001b[39m     _add_op_to_registry,\n\u001b[32m     14\u001b[39m     _convert_out_params,\n\u001b[32m     15\u001b[39m     global_decomposition_table,\n\u001b[32m     16\u001b[39m     meta_table,\n\u001b[32m     17\u001b[39m )\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_ops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpOverload\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_prims\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _prim_elementwise_meta, ELEMENTWISE_PRIM_TYPE_PROMOTION_KIND\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/torch/_decomp/__init__.py:276\u001b[39m\n\u001b[32m    272\u001b[39m             decompositions.pop(op, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    275\u001b[39m \u001b[38;5;66;03m# populate the table\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_decomp\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdecompositions\u001b[39;00m\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_refs\u001b[39;00m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcore_aten_decompositions\u001b[39m() -> \u001b[33m\"\u001b[39m\u001b[33mCustomDecompTable\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/torch/_decomp/decompositions.py:16\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_meta_registrations\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_prims\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mprims\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_prims_common\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mF\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/torch/_prims/__init__.py:525\u001b[39m\n\u001b[32m    517\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[32m    520\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    521\u001b[39m \u001b[38;5;66;03m# Elementwise unary operations\u001b[39;00m\n\u001b[32m    522\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m525\u001b[39m \u001b[38;5;28mabs\u001b[39m = \u001b[43m_make_elementwise_unary_prim\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mabs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimpl_aten\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mabs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtype_promotion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mELEMENTWISE_PRIM_TYPE_PROMOTION_KIND\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCOMPLEX_TO_FLOAT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    530\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    532\u001b[39m acos = _make_elementwise_unary_prim(\n\u001b[32m    533\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33macos\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    534\u001b[39m     impl_aten=torch.acos,\n\u001b[32m    535\u001b[39m     doc=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    536\u001b[39m     type_promotion=ELEMENTWISE_PRIM_TYPE_PROMOTION_KIND.DEFAULT,\n\u001b[32m    537\u001b[39m )\n\u001b[32m    539\u001b[39m acosh = _make_elementwise_unary_prim(\n\u001b[32m    540\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33macosh\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    541\u001b[39m     impl_aten=torch.acosh,\n\u001b[32m    542\u001b[39m     doc=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    543\u001b[39m     type_promotion=ELEMENTWISE_PRIM_TYPE_PROMOTION_KIND.DEFAULT,\n\u001b[32m    544\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/torch/_prims/__init__.py:493\u001b[39m, in \u001b[36m_make_elementwise_unary_prim\u001b[39m\u001b[34m(name, type_promotion, **kwargs)\u001b[39m\n\u001b[32m    486\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_make_elementwise_unary_prim\u001b[39m(\n\u001b[32m    487\u001b[39m     name: \u001b[38;5;28mstr\u001b[39m, *, type_promotion: ELEMENTWISE_PRIM_TYPE_PROMOTION_KIND, **kwargs\n\u001b[32m    488\u001b[39m ):\n\u001b[32m    489\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    490\u001b[39m \u001b[33;03m    Creates an elementwise unary prim.\u001b[39;00m\n\u001b[32m    491\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_make_prim\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m(Tensor self) -> Tensor\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmeta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_prim_elementwise_meta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtype_promotion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtype_promotion\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mRETURN_TYPE\u001b[49m\u001b[43m.\u001b[49m\u001b[43mNEW\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/torch/_prims/__init__.py:321\u001b[39m, in \u001b[36m_make_prim\u001b[39m\u001b[34m(schema, return_type, meta, impl_aten, doc, tags, use_old_custom_ops_api, register_conj_neg_fallthrough)\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    316\u001b[39m     mutates_args = [\n\u001b[32m    317\u001b[39m         arg.name\n\u001b[32m    318\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m cpp_schema.arguments\n\u001b[32m    319\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m arg.alias_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m arg.alias_info.is_write\n\u001b[32m    320\u001b[39m     ]\n\u001b[32m--> \u001b[39m\u001b[32m321\u001b[39m     prim_def = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlibrary\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcustom_op\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprims::\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    323\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_prim_impl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmutates_args\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmutates_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    327\u001b[39m     prim_def.register_fake(meta)\n\u001b[32m    329\u001b[39m     \u001b[38;5;66;03m# all view ops get conj/neg fallthroughs\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/torch/_library/custom_ops.py:173\u001b[39m, in \u001b[36mcustom_op\u001b[39m\u001b[34m(name, fn, mutates_args, device_types, schema)\u001b[39m\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    172\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m inner\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/torch/_library/custom_ops.py:154\u001b[39m, in \u001b[36mcustom_op.<locals>.inner\u001b[39m\u001b[34m(fn)\u001b[39m\n\u001b[32m    151\u001b[39m     schema_str = schema\n\u001b[32m    153\u001b[39m namespace, opname = name.split(\u001b[33m\"\u001b[39m\u001b[33m::\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m result = \u001b[43mCustomOpDef\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    156\u001b[39m     \u001b[38;5;66;03m# Check that schema's alias annotations match those of `mutates_args`.\u001b[39;00m\n\u001b[32m    157\u001b[39m     expected = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/torch/_library/custom_ops.py:204\u001b[39m, in \u001b[36mCustomOpDef.__init__\u001b[39m\u001b[34m(self, namespace, name, schema, fn)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28mself\u001b[39m._autocast_cpu_dtype: Optional[_dtype] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    203\u001b[39m \u001b[38;5;28mself\u001b[39m._lib = get_library_allowing_overwrite(\u001b[38;5;28mself\u001b[39m._namespace, \u001b[38;5;28mself\u001b[39m._name)\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_register_to_dispatcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[38;5;28mself\u001b[39m._disabled_kernel: \u001b[38;5;28mset\u001b[39m = \u001b[38;5;28mset\u001b[39m()\n\u001b[32m    206\u001b[39m OPDEFS[\u001b[38;5;28mself\u001b[39m._qualname] = \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/torch/_library/custom_ops.py:624\u001b[39m, in \u001b[36mCustomOpDef._register_to_dispatcher\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    616\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    617\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThere was no fake impl registered for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    618\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThis is necessary for torch.compile/export/fx tracing to work. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    619\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPlease use `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m._init_fn.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.register_fake` to add an \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    620\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfake impl.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    621\u001b[39m         )\n\u001b[32m    622\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._abstract_fn(*args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m624\u001b[39m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_register_fake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacklevel\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    626\u001b[39m autograd_impl = autograd.make_autograd_impl(\u001b[38;5;28mself\u001b[39m._opoverload, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    627\u001b[39m lib.impl(\u001b[38;5;28mself\u001b[39m._name, autograd_impl, \u001b[33m\"\u001b[39m\u001b[33mAutograd\u001b[39m\u001b[33m\"\u001b[39m, with_keyset=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/torch/library.py:193\u001b[39m, in \u001b[36mLibrary._register_fake\u001b[39m\u001b[34m(self, op_name, fn, _stacklevel)\u001b[39m\n\u001b[32m    190\u001b[39m     _library.utils.warn_deploy()\n\u001b[32m    191\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m source = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_library\u001b[49m\u001b[43m.\u001b[49m\u001b[43mutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_source\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_stacklevel\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    194\u001b[39m frame = sys._getframe(_stacklevel)\n\u001b[32m    195\u001b[39m caller_module = inspect.getmodule(frame)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/torch/_library/utils.py:54\u001b[39m, in \u001b[36mget_source\u001b[39m\u001b[34m(stacklevel)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_source\u001b[39m(stacklevel: \u001b[38;5;28mint\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m     46\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get a string that represents the caller.\u001b[39;00m\n\u001b[32m     47\u001b[39m \n\u001b[32m     48\u001b[39m \u001b[33;03m    Example: \"/path/to/foo.py:42\"\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     52\u001b[39m \u001b[33;03m    etc.\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     frame = \u001b[43minspect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetframeinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_getframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstacklevel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     source = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mframe.filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mframe.lineno\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m source\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/inspect.py:1697\u001b[39m, in \u001b[36mgetframeinfo\u001b[39m\u001b[34m(frame, context)\u001b[39m\n\u001b[32m   1695\u001b[39m start = lineno - \u001b[32m1\u001b[39m - context//\u001b[32m2\u001b[39m\n\u001b[32m   1696\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1697\u001b[39m     lines, lnum = \u001b[43mfindsource\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1698\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[32m   1699\u001b[39m     lines = index = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/inspect.py:1075\u001b[39m, in \u001b[36mfindsource\u001b[39m\u001b[34m(object)\u001b[39m\n\u001b[32m   1072\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (file.startswith(\u001b[33m'\u001b[39m\u001b[33m<\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m file.endswith(\u001b[33m'\u001b[39m\u001b[33m>\u001b[39m\u001b[33m'\u001b[39m)):\n\u001b[32m   1073\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33msource code not available\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1075\u001b[39m module = \u001b[43mgetmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1076\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m module:\n\u001b[32m   1077\u001b[39m     lines = linecache.getlines(file, module.\u001b[34m__dict__\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/inspect.py:1001\u001b[39m, in \u001b[36mgetmodule\u001b[39m\u001b[34m(object, _filename)\u001b[39m\n\u001b[32m    998\u001b[39m         f = getabsfile(module)\n\u001b[32m    999\u001b[39m         \u001b[38;5;66;03m# Always map to the name the module knows itself by\u001b[39;00m\n\u001b[32m   1000\u001b[39m         modulesbyfile[f] = modulesbyfile[\n\u001b[32m-> \u001b[39m\u001b[32m1001\u001b[39m             \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrealpath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m] = module.\u001b[34m__name__\u001b[39m\n\u001b[32m   1002\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m modulesbyfile:\n\u001b[32m   1003\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m sys.modules.get(modulesbyfile[file])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/posixpath.py:415\u001b[39m, in \u001b[36mrealpath\u001b[39m\u001b[34m(filename, strict)\u001b[39m\n\u001b[32m    412\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the canonical path of the specified filename, eliminating any\u001b[39;00m\n\u001b[32m    413\u001b[39m \u001b[33;03msymbolic links encountered in the path.\"\"\"\u001b[39;00m\n\u001b[32m    414\u001b[39m     filename = os.fspath(filename)\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     path, ok = \u001b[43m_joinrealpath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    416\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m abspath(path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/posixpath.py:435\u001b[39m, in \u001b[36m_joinrealpath\u001b[39m\u001b[34m(path, rest, strict, seen)\u001b[39m\n\u001b[32m    432\u001b[39m     path = sep\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m rest:\n\u001b[32m--> \u001b[39m\u001b[32m435\u001b[39m     name, _, rest = rest.partition(sep)\n\u001b[32m    436\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m name \u001b[38;5;129;01mor\u001b[39;00m name == curdir:\n\u001b[32m    437\u001b[39m         \u001b[38;5;66;03m# current dir\u001b[39;00m\n\u001b[32m    438\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 安裝：\n",
    "# pip install PyPDF2 easyocr pdf2image\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "from pdf2image import convert_from_path\n",
    "import easyocr, numpy as np, json, traceback\n",
    "from pathlib import Path\n",
    "\n",
    "# 初始化 OCR\n",
    "reader = easyocr.Reader(['ch_tra','en'], gpu=True)\n",
    "\n",
    "# 資料夾設定\n",
    "PDF_DIRS = [Path(\"./pdfs/finance\"), Path(\"./pdfs/insurance\")]\n",
    "OUTPUT = Path(\"./outputs/structured_passages.jsonl\")\n",
    "OUTPUT.parent.mkdir(exist_ok=True)\n",
    "\n",
    "def extract_text_pypdf2(pdf_path):\n",
    "    try:\n",
    "        rdr = PdfReader(str(pdf_path))\n",
    "        texts = []\n",
    "        for page in rdr.pages:\n",
    "            t = page.extract_text()\n",
    "            if t:\n",
    "                texts.append(t.replace(\"\\n\", \" \"))\n",
    "        return \" \".join(texts)\n",
    "    except Exception as e:\n",
    "        print(f\"PyPDF2 failed on {pdf_path}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return \"\"\n",
    "\n",
    "def fallback_ocr(pdf_path):\n",
    "    try:\n",
    "        imgs = convert_from_path(str(pdf_path), dpi=300)\n",
    "        all_t = []\n",
    "        for img in imgs:\n",
    "            arr = np.array(img)\n",
    "            res = reader.readtext(arr)\n",
    "            txt = \" \".join([r[1] for r in res if r[1].strip()])\n",
    "            if txt:\n",
    "                all_t.append(txt)\n",
    "        return \" \".join(all_t)\n",
    "    except Exception as e:\n",
    "        print(f\"OCR failed on {pdf_path}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return \"\"\n",
    "\n",
    "with open(OUTPUT, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for pdf_dir in PDF_DIRS:\n",
    "        if not pdf_dir.exists(): continue\n",
    "        for pdf in pdf_dir.glob(\"*.pdf\"):\n",
    "            print(f\"\\nProcessing {pdf.name}…\")\n",
    "            text = extract_text_pypdf2(pdf)\n",
    "            source = \"pypdf2\"\n",
    "            if len(text.strip()) < 40:\n",
    "                print(f\" → too little ({len(text)} chars), OCR fallback…\")\n",
    "                text = fallback_ocr(pdf)\n",
    "                source = \"easyocr\"\n",
    "            if text.strip():\n",
    "                rec = {\n",
    "                    \"pid\": pdf.stem,\n",
    "                    \"page\": 0,\n",
    "                    \"bbox\": None,\n",
    "                    \"text\": text.strip(),\n",
    "                    \"source\": source\n",
    "                }\n",
    "                fout.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "                print(f\" ✓ extracted {len(text)} chars via {source}\")\n",
    "            else:\n",
    "                print(f\" ❌ still empty for {pdf.name}\")\n",
    "\n",
    "print(f\"\\nDone! Results in {OUTPUT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93703d3",
   "metadata": {},
   "source": [
    "## Step 3: Hugging Face Translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ce7643d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MarianMTModel, MarianTokenizer\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from opencc import OpenCC  # 新增：簡轉繁\n",
    "\n",
    "# 下載 HuggingFace 模型 (英文 ➔ 中文)\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-zh\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# 初始化簡轉繁工具\n",
    "cc = OpenCC('s2t')  # Simplified Chinese ➔ Traditional Chinese\n",
    "\n",
    "# 載入 queries\n",
    "with open(\"/content/NTCIR-18-CLIR-pipeline-team6939/data/translated_query.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    queries = json.load(f)\n",
    "\n",
    "# 設定 cache 檔案路徑（根據 model 名稱自動命名）\n",
    "cache_path = f\"/content/NTCIR-18-CLIR-pipeline-team6939/outputs/translated_cache_{model_name.replace('/', '_')}.json\"\n",
    "if os.path.exists(cache_path):\n",
    "    with open(cache_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        translated_cache = json.load(f)\n",
    "else:\n",
    "    translated_cache = {}\n",
    "\n",
    "# 定義 NMT 翻譯函式（支援 cache + 自訂是否轉繁體）\n",
    "def translate_with_nmt(query_en, convert_to_traditional=True):\n",
    "    if query_en in translated_cache:\n",
    "        return translated_cache[query_en]\n",
    "    try:\n",
    "        inputs = tokenizer(query_en, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            translated = model.generate(**inputs, max_length=512, num_beams=5)\n",
    "        result = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "        if convert_to_traditional:\n",
    "            result = cc.convert(result)  # 自動簡體轉繁體\n",
    "        translated_cache[query_en] = result\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Translation error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# 執行批次翻譯\n",
    "translated_output = []\n",
    "for item in tqdm(queries):\n",
    "    zh = translate_with_nmt(item[\"query_en\"], convert_to_traditional=True)  # 這邊可以控管要不要轉繁體\n",
    "    item[\"query_zh_nmt\"] = zh\n",
    "    translated_output.append(item)\n",
    "\n",
    "# 存檔：翻譯後查詢 + 快取\n",
    "os.makedirs(\"/content/NTCIR-18-CLIR-pipeline-team6939/outputs\", exist_ok=True)\n",
    "\n",
    "with open(\"/content/NTCIR-18-CLIR-pipeline-team6939/outputs/translated_query_nmt.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(translated_output, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "with open(cache_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(translated_cache, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"✅ HuggingFace NMT 簡繁轉換翻譯完成並成功快取保存。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f3a05c",
   "metadata": {},
   "source": [
    "## Step 4: Run Retrieval (4 Models with Runtime Logging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aec4f751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Running BM25 baseline...\n",
      "[⏱️] BM25 baseline took 0.18 seconds.\n",
      "\n",
      "🚀 Running BM25 + Chinese BERT reranker...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/chiuyiting/Documents/GitHub/NTCIR-18-CLIR-pipeline-team6939/src/retrievers/bm25_only.py\", line 28, in <module>\n",
      "    with open(PASSAGE_PATH, 'r', encoding='utf-8') as f:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'NTCIR-18-CLIR-pipeline-team6939/outputs/runs/structured_passages.jsonl'\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/chiuyiting/Documents/GitHub/NTCIR-18-CLIR-pipeline-team6939/src/reranker/reranker_zhbert.py\", line 23, in <module>\n",
      "    with open(PASSAGE_PATH, 'r', encoding='utf-8') as f:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/content/NTCIR-18-CLIR-pipeline-team6939/outputs/runs/structured_passages.jsonl'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[⏱️] BM25 + Chinese BERT reranker took 3.41 seconds.\n",
      "\n",
      "🚀 Running Multilingual Dual Encoder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name ./models/labse. Creating a new one with mean pooling.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/chiuyiting/Documents/GitHub/NTCIR-18-CLIR-pipeline-team6939/src/retrievers/dual_encoder_dense.py\", line 29, in <module>\n",
      "    with open(PASSAGE_PATH, 'r', encoding='utf-8') as f:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'outputs/runs/structured_passages.jsonl'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[⏱️] Multilingual Dual Encoder took 5.68 seconds.\n",
      "\n",
      "🚀 Running Cross Encoder Reranker...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/transformers/utils/hub.py\", line 424, in cached_files\n",
      "    hf_hub_download(\n",
      "  File \"/Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py\", line 106, in _inner_fn\n",
      "    validate_repo_id(arg_value)\n",
      "  File \"/Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py\", line 154, in validate_repo_id\n",
      "    raise HFValidationError(\n",
      "huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/content/models/cross_encoder'. Use `repo_type` argument if needed.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/chiuyiting/Documents/GitHub/NTCIR-18-CLIR-pipeline-team6939/src/reranker/cross_encoder_multilingual.py\", line 22, in <module>\n",
      "    tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py\", line 946, in from_pretrained\n",
      "    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py\", line 778, in get_tokenizer_config\n",
      "    resolved_config_file = cached_file(\n",
      "                           ^^^^^^^^^^^^\n",
      "  File \"/Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/transformers/utils/hub.py\", line 266, in cached_file\n",
      "    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/transformers/utils/hub.py\", line 470, in cached_files\n",
      "    resolved_files = [\n",
      "                     ^\n",
      "  File \"/Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/transformers/utils/hub.py\", line 471, in <listcomp>\n",
      "    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision) for filename in full_filenames\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/transformers/utils/hub.py\", line 134, in _get_cache_file_to_return\n",
      "    resolved_file = try_to_load_from_cache(path_or_repo_id, full_filename, cache_dir=cache_dir, revision=revision)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py\", line 106, in _inner_fn\n",
      "    validate_repo_id(arg_value)\n",
      "  File \"/Users/chiuyiting/Documents/GitHub/NTCIR-pipeline/myenv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py\", line 154, in validate_repo_id\n",
      "    raise HFValidationError(\n",
      "huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/content/models/cross_encoder'. Use `repo_type` argument if needed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[⏱️] Cross Encoder Reranker took 2.29 seconds.\n",
      "\n",
      "🧪 Retrieval Runtime Summary:\n",
      "BM25 baseline                           : 0.18 seconds\n",
      "BM25 + Chinese BERT reranker            : 3.41 seconds\n",
      "Multilingual Dual Encoder               : 5.68 seconds\n",
      "Cross Encoder Reranker                  : 2.29 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#%cd ./NTCIR-18-CLIR-pipeline-team6939\n",
    "from run_all_retrievals import run_all_retrievals\n",
    "run_all_retrievals()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc570911",
   "metadata": {},
   "source": [
    "#### (fine-tune done ✅) 目前 zhbert reranker 跑不出來，但因為你說要 fine tune 所以我先跳過他 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad503e56",
   "metadata": {},
   "source": [
    "## Step 5: 合併結果成 retrieval_rankings.json ，然後評分（評分的 code 有大改，因為原本切太細了，等你修好之後 evaluate 也要再改） \n",
    "### 4/29 更：evaluate 改了✅ : MRR 現在多了 cutoff 的參數 （MRR@10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fdcd8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping bm25_only: file not found.\n",
      "⚠️ Skipping dense_dual_encoder: file not found.\n",
      "⚠️ Skipping cross_encoder: file not found.\n",
      "✅ grouped rankings saved to outputs/runs/retrieval_rankings.json\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# ────────────────────────────────────────────────\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# 4️⃣ 評估所有模型（MRR@10, Recall@10/100, NDCG@10/100）\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# ────────────────────────────────────────────────\u001b[39;00m\n\u001b[32m     52\u001b[39m sys.path.append(\u001b[38;5;28mstr\u001b[39m(BASE / \u001b[33m\"\u001b[39m\u001b[33msrc\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mevaluation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mevaluation_summary\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m evaluate_all_models\n\u001b[32m     55\u001b[39m df = evaluate_all_models(\n\u001b[32m     56\u001b[39m     ranking_path=\u001b[38;5;28mstr\u001b[39m(OUT_RANK),\n\u001b[32m     57\u001b[39m     ground_truth_path=\u001b[38;5;28mstr\u001b[39m(GROUND_TRUTH),\n\u001b[32m     58\u001b[39m     output_csv_path=\u001b[38;5;28mstr\u001b[39m(CSV_OUT),\n\u001b[32m     59\u001b[39m     ks=[\u001b[32m10\u001b[39m, \u001b[32m100\u001b[39m]\n\u001b[32m     60\u001b[39m )\n\u001b[32m     61\u001b[39m df\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/NTCIR-18-CLIR-pipeline-team6939/src/evaluation/evaluation_summary.py:2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mevaluation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mevaluation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compute_mrr, compute_recall_at_k, compute_ndcg_at_k\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_json\u001b[39m(path):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# ────────────────────────────────────────────────\n",
    "# 1️⃣ Setup Path\n",
    "# ────────────────────────────────────────────────\n",
    "BASE         = Path(\"./\")  # Colab = 根目錄\n",
    "RUNS_DIR     = BASE / \"outputs\" / \"runs\"\n",
    "OUT_RANK     = RUNS_DIR / \"retrieval_rankings.json\"\n",
    "GROUND_TRUTH = BASE / \"data\" / \"ground_truths_example.json\"\n",
    "CSV_OUT      = BASE / \"outputs\" / \"evaluation_summary.csv\"\n",
    "\n",
    "RUNS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ────────────────────────────────────────────────\n",
    "# 2️⃣ 支援多模型（只匯入已完成的 jsonl）\n",
    "# ────────────────────────────────────────────────\n",
    "retrieval = {}\n",
    "available_models = [\n",
    "    \"bm25_only_query_zh_nmt\",\n",
    "    \"bm25_rerank_query_zh_nmt\",\n",
    "    \"bm25_only_query\",\n",
    "    \"bm25_rerank_query\",\n",
    "    \"dense_dual_encoder\",\n",
    "    \"cross_encoder_rerank\"\n",
    "]\n",
    "\n",
    "for model_name in available_models:\n",
    "    fn = RUNS_DIR / f\"{model_name}.jsonl\"\n",
    "    if not fn.exists():\n",
    "        print(f\"⚠️ Skipping {model_name}: file not found.\")\n",
    "        continue\n",
    "\n",
    "    qid_to_pids = defaultdict(list)\n",
    "    with open(fn, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            r = json.loads(line)\n",
    "            pid = str(r[\"pid\"]).split(\"_\")[0]  # ✨ 只取前半段以對應 ground truth\n",
    "            qid_to_pids[str(r[\"qid\"])].append(pid)\n",
    "    retrieval[model_name] = qid_to_pids\n",
    "\n",
    "# ────────────────────────────────────────────────\n",
    "# 3️⃣ 儲存為 retrieval_rankings.json\n",
    "# ────────────────────────────────────────────────\n",
    "with open(OUT_RANK, 'w', encoding='utf-8') as f:\n",
    "    json.dump(retrieval, f, indent=2, ensure_ascii=False)\n",
    "print(f\"✅ grouped rankings saved to {OUT_RANK}\")\n",
    "\n",
    "# ────────────────────────────────────────────────\n",
    "# 4️⃣ 評估所有模型（MRR@10, Recall@10/100, NDCG@10/100）\n",
    "# ────────────────────────────────────────────────\n",
    "sys.path.append(str(BASE / \"src\"))\n",
    "from evaluation.evaluation_summary import evaluate_all_models\n",
    "\n",
    "df = evaluate_all_models(\n",
    "    ranking_path=str(OUT_RANK),\n",
    "    ground_truth_path=str(GROUND_TRUTH),\n",
    "    output_csv_path=str(CSV_OUT),\n",
    "    ks=[10, 100]\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb3b0f8",
   "metadata": {},
   "source": [
    "                Model  MRR  Recall@10  NDCG@10  Recall@100  NDCG@100\n",
    "0           bm25_only  0.0        0.0      0.0         0.0       0.0\n",
    "1  dense_dual_encoder  0.0        0.0      0.0         0.0       0.0\n",
    "2       cross_encoder  0.0        0.0      0.0         0.0       0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73572e32",
   "metadata": {},
   "source": [
    "## Step 6: Translation Error Impact Analysis (還沒試過)\n",
    "###  4/29 2100更：我也更新了這段 +translate error_analysis的code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022e41cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.analysis.translate_error_analysis import extract_translation_impact\n",
    "\n",
    "impact = extract_translation_impact(\n",
    "    queries_path=\"data/translated_query.json\",\n",
    "    predictions_path=\"outputs/runs/retrieval_rankings.json\",\n",
    "    ground_truth_path=\"data/ground_truths_example.json\"\n",
    ")\n",
    "\n",
    "for category, group in impact.items():\n",
    "    print(f\"\\n== {category.upper()} ({len(group)} samples) ==\")\n",
    "    for qid, en, zh, pred, gt in group[:1]:  # 每個 category 顯示1個 example\n",
    "        print(f\"QID: {qid}\\nEN: {en}\\nZH(NMT): {zh}\\nPRED_TOPK: {pred}\\nGT: {gt}\\n---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
