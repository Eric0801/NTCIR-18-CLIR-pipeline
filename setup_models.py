from pathlib import Path
from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification
from huggingface_hub import login
import os

# â¬‡ï¸ å…ˆè®€å– .env è£¡é¢çš„ HF_TOKENï¼ˆå¦‚æœå­˜åœ¨çš„è©±ï¼‰
if os.path.exists(".env"):
    from dotenv import load_dotenv
    load_dotenv()
    hf_token = os.getenv("HF_TOKEN")
    if hf_token:
        login(token=hf_token)
        print("[âœ“] HuggingFace private token loaded.")
    else:
        print("âš ï¸ .env file found, but HF_TOKEN is empty. Will try anonymous access.")
else:
    print("âš ï¸ No .env file found. Will try anonymous access.")

# â¬‡ï¸ è¨­å®šè¦ä¸‹è¼‰çš„æ¨¡å‹
MODEL_CONFIGS = {
    "zhbert": {
        "hf_id": "hfl/chinese-roberta-wwm-ext",
        "model_cls": AutoModelForSequenceClassification
    },
    "labse": {
        "hf_id": "sentence-transformers/LaBSE",
        "model_cls": AutoModel
    },
    "cross_encoder": {
        "hf_id": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        "model_cls": AutoModelForSequenceClassification
    },
    "zhbert_finetuned": {   # ğŸ‘ˆ æ–°å¢ä½ ä¸Šå‚³åˆ° Hugging Face çš„ fine-tuned model
        "hf_id": "eatingchew/zhbert_finetuned",
        "model_cls": AutoModelForSequenceClassification
    }
}

BASE_DIR = Path("models")

def download_model(name, config):
    save_path = BASE_DIR / name
    if save_path.exists() and any(save_path.iterdir()):
        print(f"[âœ“] {name} already exists, skipping.")
        return
    print(f"â†“ Downloading {name} from {config['hf_id']}...")
    tokenizer = AutoTokenizer.from_pretrained(config["hf_id"])
    model = config["model_cls"].from_pretrained(config["hf_id"])
    save_path.mkdir(parents=True, exist_ok=True)
    tokenizer.save_pretrained(save_path)
    model.save_pretrained(save_path)
    print(f"[âœ“] Saved to {save_path}")

if __name__ == "__main__":
    for name, cfg in MODEL_CONFIGS.items():
        download_model(name, cfg)