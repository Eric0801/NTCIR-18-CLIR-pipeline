
# ğŸ“˜ Cross-Lingual IR for Traditional Chinese Financial Documents

This is the official demo package for our NTCIR-18 AI Cup submission:

> **"Translation or Multilingual Retrieval? Evaluating Cross-Lingual Search Strategies for Traditional Chinese Financial Documents"**

---

## ğŸ“¦ Folder Structure

```
clir_pipeline/
â”œâ”€â”€ CLIR_Reviewer_Demo_Full.ipynb   â† ğŸ“˜ Main demo notebook
â”œâ”€â”€ setup_models.py                 â† ğŸ”§ Auto-download models to /models
â”œâ”€â”€ run_all_retrievals.py          â† ğŸ” Run 4 retrieval models with timing
â”‚
â”œâ”€â”€ models/                         â† ğŸ§  Local models (auto-downloaded)
â”‚   â”œâ”€â”€ zhbert/                     â† Chinese BERT reranker
â”‚   â”œâ”€â”€ labse/                      â† LaBSE dense encoder
â”‚   â””â”€â”€ cross_encoder/             â† Multilingual cross encoder
â”‚
â”œâ”€â”€ data/                           â† ğŸ“‚ Query + Ground truth + Dictionary
â”‚   â”œâ”€â”€ translated_query.json
â”‚   â”œâ”€â”€ ground_truths_example.json
â”‚   â””â”€â”€ userdict.txt               â† (optional) for jieba customization
â”‚
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ structured_passages.jsonl  â† ğŸ“„ Extracted paragraphs from PDFs
â”‚   â”œâ”€â”€ runs/
â”‚   â”‚   â”œâ”€â”€ bm25_only.jsonl
â”‚   â”‚   â”œâ”€â”€ bm25_rerank.jsonl
â”‚   â”‚   â”œâ”€â”€ dense_dual_encoder.jsonl
â”‚   â”‚   â””â”€â”€ cross_encoder.jsonl
â”‚   â””â”€â”€ evaluation_summary_all.csv
â”‚
â”œâ”€â”€ pdfs/                          â† ğŸ“š Source documents
â”‚   â”œâ”€â”€ faq/                       â† Contains queries and pid_map.json
â”‚   â”œâ”€â”€ finance/                   â† ~1000 PDFs
â”‚   â””â”€â”€ insurance/                 â† ~643 PDFs
â”‚
â””â”€â”€ src/
    â”œâ”€â”€ retrievers/
    â”‚   â”œâ”€â”€ bm25_only.py
    â”‚   â””â”€â”€ dual_encoder_dense.py
    â””â”€â”€ reranker/
        â”œâ”€â”€ reranker_zhbert.py
        â””â”€â”€ cross_encoder_multilingual.py
```

---

## ğŸš€ How to Run in Colab

```python
# Step 0: Download models (only first time)
!python setup_models.py

# Step 1: Run 4 retrieval models and generate `.jsonl` results
from run_all_retrievals import run_all_retrievals
run_all_retrievals()

# Step 2: Evaluate
from evaluation_summary import evaluate_all_models

# Step 3: Analyze translation impact
from translate_error_analysis import extract_translation_impact
```

---

## ğŸ“Š Evaluated Metrics

- MRR@10 / NDCG@10 / Recall@100
- Runtime (logged per model)
- Translation error sensitivity analysis

---

## âœï¸ Reviewer's Comments Addressed

- âœ… BM25 baseline added
- âœ… Runtime comparison included
- âœ… Cross-encoder added
- âœ… System flowchart (available in `assets/`)
- âœ… Translation error cases analyzed
- âœ… Citation style fixed
- âœ… Team name removed in paper

